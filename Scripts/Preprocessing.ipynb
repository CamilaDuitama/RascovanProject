{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#SRA-files-downloaded\" data-toc-modified-id=\"SRA-files-downloaded-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>SRA files downloaded</a></span></li><li><span><a href=\"#Plots-of-metadata-variables\" data-toc-modified-id=\"Plots-of-metadata-variables-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Plots of metadata variables</a></span></li><li><span><a href=\"#Homo-Sapiens-Oral-metadata\" data-toc-modified-id=\"Homo-Sapiens-Oral-metadata-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Homo Sapiens Oral metadata</a></span></li></ul></li><li><span><a href=\"#DASK\" data-toc-modified-id=\"DASK-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>DASK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Homo-sapiens-oral-(50th---5.8-GB)\" data-toc-modified-id=\"Homo-sapiens-oral-(50th---5.8-GB)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Homo sapiens oral (50th - 5.8 GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-samples\" data-toc-modified-id=\"Plot-samples-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Plot samples</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binarization-+-PCA\" data-toc-modified-id=\"Binarization-+-PCA-2.1.1.1\"><span class=\"toc-item-num\">2.1.1.1&nbsp;&nbsp;</span>Binarization + PCA</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.1.1.2\"><span class=\"toc-item-num\">2.1.1.2&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li><li><span><a href=\"#Plot-k-mers\" data-toc-modified-id=\"Plot-k-mers-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Plot k-mers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binarization-+-PCA\" data-toc-modified-id=\"Binarization-+-PCA-2.1.2.1\"><span class=\"toc-item-num\">2.1.2.1&nbsp;&nbsp;</span>Binarization + PCA</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.1.2.2\"><span class=\"toc-item-num\">2.1.2.2&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li></ul></li><li><span><a href=\"#Homo-sapiens-oral-(50th---5.8-GB)-without-human-reads\" data-toc-modified-id=\"Homo-sapiens-oral-(50th---5.8-GB)-without-human-reads-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Homo sapiens oral (50th - 5.8 GB) without human reads</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-k-mers\" data-toc-modified-id=\"Plot-k-mers-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot k-mers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li></ul></li><li><span><a href=\"#Homo-sapiens-oral-(25th---12GB)\" data-toc-modified-id=\"Homo-sapiens-oral-(25th---12GB)-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Homo sapiens oral (25th - 12GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-k-mers\" data-toc-modified-id=\"Plot-k-mers-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Plot k-mers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.3.1.1\"><span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li></ul></li><li><span><a href=\"#aSGenomes\" data-toc-modified-id=\"aSGenomes-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>aSGenomes</a></span></li><li><span><a href=\"#Combined-datasets\" data-toc-modified-id=\"Combined-datasets-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Combined datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Every-50th-k-mer-(8.3GB)\" data-toc-modified-id=\"Every-50th-k-mer-(8.3GB)-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Every 50th k-mer (8.3GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binarization-+-PCA-on-entire-dataset\" data-toc-modified-id=\"Binarization-+-PCA-on-entire-dataset-2.5.1.1\"><span class=\"toc-item-num\">2.5.1.1&nbsp;&nbsp;</span>Binarization + PCA on entire dataset</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA-on-informed-dataset\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-informed-dataset-2.5.1.2\"><span class=\"toc-item-num\">2.5.1.2&nbsp;&nbsp;</span>Standard scaling + PCA on informed dataset</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA-on-entire-dataset\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-entire-dataset-2.5.1.3\"><span class=\"toc-item-num\">2.5.1.3&nbsp;&nbsp;</span>Standard scaling + PCA on entire dataset</a></span></li><li><span><a href=\"#Kernel-PCA-on--arbitrary-subset\" data-toc-modified-id=\"Kernel-PCA-on--arbitrary-subset-2.5.1.4\"><span class=\"toc-item-num\">2.5.1.4&nbsp;&nbsp;</span>Kernel PCA on  arbitrary subset</a></span></li><li><span><a href=\"#Kernel-PCA-on-informed-subset\" data-toc-modified-id=\"Kernel-PCA-on-informed-subset-2.5.1.5\"><span class=\"toc-item-num\">2.5.1.5&nbsp;&nbsp;</span>Kernel PCA on informed subset</a></span></li></ul></li><li><span><a href=\"#Whole-dataset-(412GB)\" data-toc-modified-id=\"Whole-dataset-(412GB)-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Whole dataset (412GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling-+-PCA-on-informative-k-mers\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-informative-k-mers-2.5.2.1\"><span class=\"toc-item-num\">2.5.2.1&nbsp;&nbsp;</span>Standard scaling + PCA on informative k-mers</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Plots-for-meeting-on-the-14/04/2021\" data-toc-modified-id=\"Plots-for-meeting-on-the-14/04/2021-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Plots for meeting on the 14/04/2021</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram\" data-toc-modified-id=\"Standard-scaling+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Standard scaling+ PCA of a subsample that is representative of each group in the Venn diagram</a></span></li><li><span><a href=\"#Binarization-+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram\" data-toc-modified-id=\"Binarization-+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Binarization + PCA of a subsample that is representative of each group in the Venn diagram</a></span></li><li><span><a href=\"#Standard-scaling-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data\" data-toc-modified-id=\"Standard-scaling-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Standard scaling + Kernel PCA (RBF kernel) on 0.1% of the data</a></span></li><li><span><a href=\"#Binarization-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data\" data-toc-modified-id=\"Binarization-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Binarization + Kernel PCA (RBF kernel) on 0.1% of the data</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA-on-exclusive-k-mers-without-outliers\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-exclusive-k-mers-without-outliers-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Standard scaling + PCA on exclusive k-mers without outliers</a></span></li><li><span><a href=\"#Binarization-+-PCA-on-exclusive-k-mers-without-outliers\" data-toc-modified-id=\"Binarization-+-PCA-on-exclusive-k-mers-without-outliers-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Binarization + PCA on exclusive k-mers without outliers</a></span></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>Naive-Bayes</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aMetagenomes=pd.read_table(\"Metadata/aMetagenomes/SRR_aMetagenomes.txt\", names=[\"Accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_SRStoSRR=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes_SRStoSRR.tsv\",names=[\"archive_accession\",\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aMetagenomes_metadata=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes.txt\")\n",
    "#Create one row per archive_accession\n",
    "aMetagenomes_metadata=aMetagenomes_metadata.assign(archive_accession=aMetagenomes_metadata['archive_accession'].str.split(',')).explode('archive_accession')\n",
    "aMetagenomes_metadata=aMetagenomes_metadata.reset_index()\n",
    "aMetagenomes_metadata=aMetagenomes_metadata[aMetagenomes_metadata[\"archive\"]==\"SRA\"]\n",
    "aMetagenomes_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>project_name</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_doi</th>\n",
       "      <th>site_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geo_loc_name</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>sample_host</th>\n",
       "      <th>sample_age</th>\n",
       "      <th>sample_age_doi</th>\n",
       "      <th>community_type</th>\n",
       "      <th>material</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>archive</th>\n",
       "      <th>archive_project</th>\n",
       "      <th>archive_accession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>G12</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  project_name  publication_year  publication_doi site_name  latitude  \\\n",
       "0      1  Warinner2014              2014  10.1038/ng.2906   Dalheim    51.565   \n",
       "1      1  Warinner2014              2014  10.1038/ng.2906   Dalheim    51.565   \n",
       "2      1  Warinner2014              2014  10.1038/ng.2906   Dalheim    51.565   \n",
       "3      1  Warinner2014              2014  10.1038/ng.2906   Dalheim    51.565   \n",
       "4      2  Warinner2014              2014  10.1038/ng.2906   Dalheim    51.565   \n",
       "\n",
       "   longitude geo_loc_name sample_name   sample_host  sample_age  \\\n",
       "0       8.84      Germany         B61  Homo sapiens         900   \n",
       "1       8.84      Germany         B61  Homo sapiens         900   \n",
       "2       8.84      Germany         B61  Homo sapiens         900   \n",
       "3       8.84      Germany         B61  Homo sapiens         900   \n",
       "4       8.84      Germany         G12  Homo sapiens         900   \n",
       "\n",
       "    sample_age_doi community_type         material  collection_date archive  \\\n",
       "0  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "1  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "2  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "3  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "4  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "\n",
       "  archive_project archive_accession  \n",
       "0     PRJNA216965         SRS473742  \n",
       "1     PRJNA216965         SRS473743  \n",
       "2     PRJNA216965         SRS473744  \n",
       "3     PRJNA216965         SRS473745  \n",
       "4     PRJNA216965         SRS473747  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aMetagenomes_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter aMetagenomes_metadata with only files that were downloaded (SRA files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRA files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aMetagenomes=pd.read_table(\"Metadata/aMetagenomes/SRR_aMetagenomes.txt\",names=[\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.merge(aMetagenomes_SRStoSRR,SRR_aMetagenomes,on=\"SRR_accession\",how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_metadata=pd.merge(aMetagenomes_metadata,aMetagenomes_SRStoSRR,on=\"archive_accession\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of metadata variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"sample_age\",title=\"Sample age\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"sample_host\",title=\"Sample host\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"community_type\",title=\"Community type\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"material\",title=\"material\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(aMetagenomes_metadata, lat=\"latitude\", lon=\"longitude\", hover_name=\"geo_loc_name\",\n",
    "                        hover_data=[\"site_name\",\"sample_host\",\"sample_age\",\"community_type\",\"SRR_accession\"],\n",
    "                        color_discrete_sequence=[\"red\"], zoom=2, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo Sapiens Oral metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I select the SRR_accessions from the aMetagenomes_metadata DF where the sample_host is Homo sapiens and the community type is oral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_metadata=aMetagenomes_metadata[(aMetagenomes_metadata[\"sample_host\"]==\"Homo sapiens\")\\\n",
    "                                  & (aMetagenomes_metadata[\"community_type\"]==\"oral\")]\n",
    "HS_Oral_metadata=HS_Oral_metadata.reset_index()\n",
    "SRR_HS_Oral=[\"Kmer\"]+list(HS_Oral_metadata[\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>project_name</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_doi</th>\n",
       "      <th>site_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geo_loc_name</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>sample_host</th>\n",
       "      <th>sample_age</th>\n",
       "      <th>sample_age_doi</th>\n",
       "      <th>community_type</th>\n",
       "      <th>material</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>archive</th>\n",
       "      <th>archive_project</th>\n",
       "      <th>archive_accession</th>\n",
       "      <th>SRR_accession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473742</td>\n",
       "      <td>SRR957738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473743</td>\n",
       "      <td>SRR957739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473744</td>\n",
       "      <td>SRR957740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>B61</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473745</td>\n",
       "      <td>SRR957741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Warinner2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>Dalheim</td>\n",
       "      <td>51.565</td>\n",
       "      <td>8.84</td>\n",
       "      <td>Germany</td>\n",
       "      <td>G12</td>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>900</td>\n",
       "      <td>10.1038/ng.2906</td>\n",
       "      <td>oral</td>\n",
       "      <td>dental calculus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SRA</td>\n",
       "      <td>PRJNA216965</td>\n",
       "      <td>SRS473747</td>\n",
       "      <td>SRR957742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  project_name  publication_year  publication_doi site_name  \\\n",
       "0        0      1  Warinner2014              2014  10.1038/ng.2906   Dalheim   \n",
       "1        1      1  Warinner2014              2014  10.1038/ng.2906   Dalheim   \n",
       "2        2      1  Warinner2014              2014  10.1038/ng.2906   Dalheim   \n",
       "3        3      1  Warinner2014              2014  10.1038/ng.2906   Dalheim   \n",
       "4        4      2  Warinner2014              2014  10.1038/ng.2906   Dalheim   \n",
       "\n",
       "   latitude  longitude geo_loc_name sample_name   sample_host  sample_age  \\\n",
       "0    51.565       8.84      Germany         B61  Homo sapiens         900   \n",
       "1    51.565       8.84      Germany         B61  Homo sapiens         900   \n",
       "2    51.565       8.84      Germany         B61  Homo sapiens         900   \n",
       "3    51.565       8.84      Germany         B61  Homo sapiens         900   \n",
       "4    51.565       8.84      Germany         G12  Homo sapiens         900   \n",
       "\n",
       "    sample_age_doi community_type         material  collection_date archive  \\\n",
       "0  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "1  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "2  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "3  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "4  10.1038/ng.2906           oral  dental calculus              NaN     SRA   \n",
       "\n",
       "  archive_project archive_accession SRR_accession  \n",
       "0     PRJNA216965         SRS473742     SRR957738  \n",
       "1     PRJNA216965         SRS473743     SRR957739  \n",
       "2     PRJNA216965         SRS473744     SRR957740  \n",
       "3     PRJNA216965         SRS473745     SRR957741  \n",
       "4     PRJNA216965         SRS473747     SRR957742  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HS_Oral_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(HS_Oral_metadata, x=\"sample_age\",title=\"Sample age\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(HS_Oral_metadata, x=\"publication_year\",title=\"Publication year\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(HS_Oral_metadata, x=\"material\",title=\"Material\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(HS_Oral_metadata, lat=\"latitude\", lon=\"longitude\", hover_name=\"geo_loc_name\",\n",
    "                        hover_data=[\"site_name\",\"sample_host\",\"sample_age\",\"community_type\",\"SRR_accession\"],\n",
    "                        color_discrete_sequence=[\"red\"], zoom=2, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:41317</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>15</li>\n",
       "  <li><b>Memory: </b>1.46 TiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:41317' processes=5 threads=15, memory=1.46 TiB>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "#client.shutdown()\n",
    "#client.close()\n",
    "dask.config.set({'temporary_directory': '/pasteur/sonic/scratch/public/cduitama/RascovanProject/tmp/'})\n",
    "#dask.config.set({\"optimization.fuse.ave-width\": 5})\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.get_versions(check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo sapiens oral (50th - 5.8 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size = 5.8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=dd.read_table(\"kmMatrices/aMetagenomes/HS_Oral_every50th.txt\",sep=\" \",header=None,names=SRR_HS_Oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove column of k-mers\n",
    "test=test.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test=test.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "transformer = Binarizer().fit(test)  # fit does nothing.\n",
    "b_test=transformer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_test=b_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX = b_test\n",
    "pca= PCA(n_components=2)\n",
    "to_plot=pca.fit(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Samples_bPCA.csv\", to_plot.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Samples_bPCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=to_plot[0], y=to_plot[1], color=HS_Oral_metadata[\"geo_loc_name\"], \\\n",
    "                 title=\"Binarization + PCA of samples coloured based on origin <br> Dataset: H.sapiens Oral every 50th k-mer\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we standardize the features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test)\n",
    "scaled_test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=scaled_test.mean(axis=0).compute()\n",
    "b=scaled_test.std(axis=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0:10])\n",
    "print(b[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_1 = scaled_test\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_1 = PCA(n_components=2,whiten=True)\n",
    "to_plot_1=pca_1.fit(dX_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_1.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Samples_SS_PCA.csv\", to_plot_1.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_1=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Samples_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=to_plot_1[0], y=to_plot_1[1],color=HS_Oral_metadata[\"geo_loc_name\"],\n",
    "                title=\"Standard scaling + PCA of samples coloured based on location <br> Dataset: H.sapiens Oral every 50th kmer\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot k-mers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "transformer = Binarizer().fit(t_test)  # fit does nothing.\n",
    "tb_test=transformer.transform(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "dX_5 = tb_test\n",
    "pca_5 = PCA(n_components=2)\n",
    "to_plot_5=pca_5.fit(dX_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_5.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_5.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Kmers_bPCA.csv\", to_plot_5.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_5=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Kmers_bPCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n=10000\n",
    "randomlist = random.sample(range(0, to_plot_5.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_5[0,randomlist], y=to_plot_5[1,randomlist],opacity=0.5,\\\n",
    "                 title=\"Binarization + PCA of k-mers <br> Dataset: H.sapiens Oral every 50th k-mer <br>\"+\\\n",
    "                 \" Sample: \" +str(n)+\" randomly selected k-mers \")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test)\n",
    "scaled_test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_test=scaled_test.transpose()\n",
    "scaled_t_test=scaled_t_test.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_4 = scaled_t_test\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_4 = PCA(n_components=2,whiten=True)\n",
    "to_plot_4=pca_4.fit(dX_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_4.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_4.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Kmers_SS_PCA.csv\", to_plot_4.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_plot_4=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Kmers_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "import random\n",
    "randomlist = random.sample(range(0, to_plot_4.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_4[0,randomlist], y=to_plot_4[1,randomlist],opacity=0.5,\\\n",
    "                 title=\"Standard scaling + PCA of k-mers <br> Dataset: H.sapiens Oral every 50th k-mer <br>\"+\\\n",
    "                 \" Sample: \" +str(n)+\" randomly selected k-mers \")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo sapiens oral (50th - 5.8 GB) without human reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_aMetagenomes=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes_filesize.txt\", sep=\"\\s+\",\\\n",
    "                              names=[\"index\",\"one\",\"User\",\"Machine\",\"Size\",\"Month\",\"Day\",\"Time\",\"fastq\"])\n",
    "fs_aMetagenomes=fs_aMetagenomes.drop([\"index\",\"one\",\"User\",\"Machine\",\"Month\",\"Day\",\"Time\"],axis=1)\n",
    "fs_aMetagenomes=fs_aMetagenomes.drop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_aOralNonHuman=pd.read_table(\"Metadata/aOralNonHuman/aOralNonHuman_filesize.txt\", sep=\"\\s+\",\\\n",
    "                              names=[\"index\",\"one\",\"User\",\"Machine\",\"Size\",\"Month\",\"Day\",\"Time\",\"fastq\"])\n",
    "fs_aOralNonHuman=fs_aOralNonHuman.drop([\"index\",\"one\",\"User\",\"Machine\",\"Month\",\"Day\",\"Time\"],axis=1)\n",
    "fs_aOralNonHuman=fs_aOralNonHuman.drop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileSizes=fs_aOralNonHuman.merge(fs_aMetagenomes,how=\"left\",on=\"fastq\")\n",
    "FileSizes=FileSizes.rename(columns={\"Size_x\": \"After\", \"Size_y\": \"Before\"})\n",
    "FileSizes[\"Reduction\"]=FileSizes[\"Before\"] - FileSizes[\"After\"]\n",
    "FileSizes[\"After\"]=FileSizes[\"After\"]/1e9\n",
    "FileSizes[\"Before\"]=FileSizes[\"Before\"]/1e9\n",
    "FileSizes[\"Reduction\"]=FileSizes[\"Reduction\"]/1e9\n",
    "FileSizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "fig = px.histogram(FileSizes, x=\"Reduction\",title=\"Reduction in GB\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Before', x=FileSizes[\"fastq\"], y=FileSizes[\"Before\"]),\n",
    "    go.Bar(name='After', x=FileSizes[\"fastq\"], y=FileSizes[\"After\"])\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode=\"overlay\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a file size reduction of 29GB in total in the fastq files, but this represented only 1 GB reduction in the km-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_filtered50=dd.read_table(\"kmMatrices/aOralNonHuman/aOralNonHuman_every50th.txt\",sep=\" \",\\\n",
    "                      header=None,names=SRR_HS_Oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_filtered50=HS_Oral_filtered50.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_filtered50.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot k-mers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(HS_Oral_filtered50)\n",
    "scaled_ftest=scaler.transform(HS_Oral_filtered50)\n",
    "scaled_t_ftest=scaled_ftest.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_6 = scaled_t_ftest.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_6 = PCA(n_components=2,whiten=True)\n",
    "to_plot_6=pca_6.fit(dX_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_6.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_6.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/filteredKmers_SS_PCA.csv\", to_plot_6.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_6=np.genfromtxt(\"Preprocessed_data/combination/filteredKmers_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "n=10000\n",
    "#Generate 5 random numbers between 10 and 30\n",
    "randomlist = random.sample(range(0, to_plot_6.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_6[0,randomlist], y=to_plot_6[1,randomlist],opacity=0.3,\\\n",
    "                title=\"Standard scaling + PCA of kmers <br> Dataset: H.sapiens Oral every 50th kmer <br> Sample: Randomly selected \" \\\n",
    "                + str(n) + \"k-mers\")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo sapiens oral (25th - 12GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=dd.read_table(\"kmMatrices/aMetagenomes/HS_Oral_every25th.txt\",sep=\" \",header=None,names=SRR_HS_Oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove column of k-mers\n",
    "test2=test2.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot k-mers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test2)\n",
    "scaled_test2=scaler.transform(test2)\n",
    "scaled_t_test2=scaled_test2.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_7 = scaled_t_test2.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_7 = PCA(n_components=2,whiten=True)\n",
    "to_plot_7=pca_7.fit(dX_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_7.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_7.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Every25th_SS_PCA.csv\", to_plot_7.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_7=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Every25th_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "randomlist = random.sample(range(0, to_plot_7.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_7[0,randomlist], y=to_plot_7[1,randomlist],opacity=0.3,\\\n",
    "                title=\"Standard scaling + PCA of kmers <br> Dataset: H.sapiens Oral every 25th kmer <br> Sample: Randomly selected \" \\\n",
    "                + str(n) + \" k-mers\")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aSGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSGenomes_metadata=pd.read_table(\"Metadata/aSGenomes/aSGenomes.txt\")\n",
    "#Create one row per archive_accession\n",
    "aSGenomes_metadata=aSGenomes_metadata.assign(archive_accession=aSGenomes_metadata['archive_accession'].str.split(',')).explode('archive_accession')\n",
    "aSGenomes_metadata=aSGenomes_metadata.reset_index()\n",
    "aSGenomes_metadata=aSGenomes_metadata.drop(\"index\",axis=1)\n",
    "aSGenomes_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSGenomes_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(aMetagenomes_metadata.archive_accession))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(aSGenomes_metadata.archive_accession))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(aSGenomes_metadata.archive_accession).intersection(set(aMetagenomes_metadata.archive_accession)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21 archive accessions that exists both in the aSGenomes dataset and the aMetagenomes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract only samples where the host is Homo sapiens and the material is calculus or tooth\n",
    "aSGenomes_HS_Oral_metadata=aSGenomes_metadata[(aSGenomes_metadata[\"sample_host\"]==\"Homo sapiens\")\n",
    "                                              &(aSGenomes_metadata[\"material\"]==(\"dental calculus\"or \"tooth\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aSGenomes_HS_Oral_metadata, x=\"singlegenome_species\",\\\n",
    "                   title=\"aSingle genome species\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined dataset refers to the K-mer matrix of abundanced built from the following dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition of the combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | Number of SRR accessions | Percentage of total number | Fastq total File size (GB) | Percentage of total size |\n",
    "| :---: | :---: | :---: | :---: | :---: | \n",
    "| aMetagenomes | 281 | 94.3% | 338.03 | 86.6% |\n",
    "| aSGenomes | 5 | 1.7% | 9.3 | 2.4% |\n",
    "| mMetagenomes | 12 | 4.0% | 43 | 11.0% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 50th k-mer (8.3GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification=pd.read_csv(\"Preprocessed_data/combination/classification.csv\")\n",
    "classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_HS_Oral_filesizes=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes_HS_Oral_filesizes.txt\",header=None,names=[\"Size\",\"File\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toG(row):\n",
    "    \n",
    "    if \"K\" in row[\"Size\"]:\n",
    "        val = row[\"Size\"].replace(\"K\",\"\")\n",
    "        val = float(val)\n",
    "        val = val/1000000\n",
    "        \n",
    "    elif \"M\" in row[\"Size\"]:\n",
    "        val = row[\"Size\"].replace(\"M\",\"\")\n",
    "        val = float(val)\n",
    "        val = val/1000\n",
    "        \n",
    "    else:\n",
    "        val = row[\"Size\"].replace(\"G\",\"\")\n",
    "        val = float(val)\n",
    "        \n",
    "    return val\n",
    "\n",
    "aMetagenomes_HS_Oral_filesizes[\"Size_GB\"]=aMetagenomes_HS_Oral_filesizes.apply(toG,axis=1)\n",
    "aMetagenomes_HS_Oral_filesizes=aMetagenomes_HS_Oral_filesizes.drop(\"Size\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_HS_Oral_filesizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aSGenomes_HS_Oral=pd.read_csv(\"Metadata/aSGenomes/aSGenomes_HS_Oral_SRR.txt\",header=None,names=[\"SRR_accession\"])\n",
    "SRR_mMetagenomes_HS_Oral=pd.read_csv(\"Metadata/mMetagenomes/Oral_HS_mMetagenomes_SRR.txt\",\\\n",
    "                                     header=None,names=[\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "SRR_HS_Oral_aSGenomes=list(SRR_aSGenomes_HS_Oral.SRR_accession)\n",
    "SRR_HS_Oral_mMetagenomes=list(SRR_mMetagenomes_HS_Oral.SRR_accession)\n",
    "names=SRR_HS_Oral+SRR_HS_Oral_aSGenomes+SRR_HS_Oral_mMetagenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aMetagenomes=len(SRR_HS_Oral_aMetagenomes)\n",
    "n_aSGenomes=len(SRR_HS_Oral_aSGenomes)\n",
    "n_mMetagenomes=len(SRR_HS_Oral_mMetagenomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=combined.drop(\"Kmer\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of times the kmer K appears in the samples of \n",
    "aMetagenomes, aSGenomes and mMetagenomes respectively?\n",
    "The kmer k is in n% of the samples of xGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "total_aMetagenomes=(combined[SRR_HS_Oral_aMetagenomes] != 0).astype(int).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aSGenomes=(combined[SRR_HS_Oral_aSGenomes] != 0).astype(int).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mMetagenomes=(combined[SRR_HS_Oral_mMetagenomes] != 0).astype(int).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_aMetagenomes=(total_aMetagenomes/n_aMetagenomes)*100\n",
    "color_aSGenomes=(total_aSGenomes/n_aSGenomes)*100\n",
    "color_mMetagenomes=(total_mMetagenomes/n_mMetagenomes)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification=pd.DataFrame()\n",
    "classification[\"Perc_aMetagenome\"]=color_aMetagenomes\n",
    "classification[\"Perc_aSGenome\"]=color_aSGenomes\n",
    "classification[\"Perc_mMetagenome\"]=color_mMetagenomes\n",
    "classification=classification.reset_index()\n",
    "classification=classification.drop(\"index\",axis=1)\n",
    "classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "classification.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.to_csv(\"Preprocessed_data/combination/classification.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification=classification.reset_index()\n",
    "classification=classification.drop(\"index\",axis=1)\n",
    "mMetagenomes=set(classification[(classification.Perc_mMetagenome > float(0))].index.values.tolist())\n",
    "aMetagenomes=set(classification[(classification.Perc_aMetagenome > float(0))].index.values.tolist())\n",
    "aSGenomes=set(classification[(classification.Perc_aSGenome > float(0))].index.values.tolist())\n",
    "intersection=set.intersection(aMetagenomes,mMetagenomes,aSGenomes)\n",
    "aMetagenomes_aSGenomes=aMetagenomes.intersection(aSGenomes)\n",
    "aMetagenomes_mMetagenomes=aMetagenomes.intersection(mMetagenomes)\n",
    "mMetagenomes_aSGenomes=mMetagenomes.intersection(aSGenomes)\n",
    "onlymMetagenomes=mMetagenomes-mMetagenomes_aSGenomes-aMetagenomes_mMetagenomes\n",
    "onlyaSGenomes=aSGenomes-mMetagenomes_aSGenomes-aMetagenomes_aSGenomes\n",
    "onlyaMetagenomes=aMetagenomes-aMetagenomes_aSGenomes-aMetagenomes_mMetagenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(mMetagenomes_aSGenomes)+len(aMetagenomes_aSGenomes)+len(aMetagenomes_mMetagenomes)-\\\n",
    "2*len(intersection)+len(onlyaSGenomes)+len(onlymMetagenomes)+len(onlyaMetagenomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "labels.append(str(round((len(onlyaMetagenomes)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(aMetagenomes_aSGenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(aMetagenomes_mMetagenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(onlymMetagenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(onlyaSGenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(mMetagenomes_aSGenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(intersection)/classification.shape[0])*100,3))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition of the combined dataset of every 50th k-mers based on assigned labels as aMetagenomes, mMetagenomes and aSGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "subsets = (1, 1, 0.2, 1, 0.2, 0.2, 0.1)\n",
    "v = venn3(subsets=subsets, set_labels=(\"aMetagenomes\",\"mMetagenomes\",\"aSGenomes\"))\n",
    "\n",
    "dummies = ['100', '101', '110', '010', '001', '011', '111']\n",
    "\n",
    "for dummie,label in zip(dummies,labels):\n",
    "    v.get_label_by_id(dummie).set_text(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_kmers=list(set.union(onlyaMetagenomes,onlyaSGenomes,onlymMetagenomes))\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(informative_kmers)/classification.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization + PCA on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "combined=combined.drop(\"Kmer\",axis=1).values\n",
    "combined_t=combined.transpose()\n",
    "transformer = Binarizer().fit(combined_t)  # fit does nothing.\n",
    "t_b_combined=transformer.transform(combined_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_10 = t_b_combined\n",
    "pca_10 = PCA(n_components=2)\n",
    "to_plot_10=pca_10.fit(dX_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_10.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_10.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_Binarized_PCA.txt\", to_plot_10.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_10=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_Binarized_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onlyaMetagenomes))\n",
    "print(len(onlyaSGenomes))\n",
    "print(len(onlymMetagenomes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "x=80000\n",
    "y=105\n",
    "z=2200\n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_10[0,aM], y=to_plot_10[1,aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_10[0,aS], y=to_plot_10[1,aS],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_10[0,mM], y=to_plot_10[1,mM],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Binarization + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Combined dataset every 50th kmer',title_y=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "\n",
    "This plot is the result of the binarization and posterior PCA of the combined dataset with every 50th k-mer.\n",
    "I only plot a subsample of the total number of kmers(0.08% of the total)\n",
    "The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA on informed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Preprocessed_data/combination/informative_kmers.txt', 'w') as f:\n",
    "    for item in [x+1 for x in informative_kmers]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_kmers=list(set.union(onlyaMetagenomes,onlyaSGenomes,onlymMetagenomes))\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers=all_kmers.iloc[informative_kmers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icombined=combined[combined[\"Kmer\"].isin(ikmers[\"Kmer\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icombined=icombined.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(icombined)\n",
    "scaled_icombined=scaler.transform(icombined)\n",
    "scaled_t_icombined=scaled_icombined.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_icombined.mean(axis=1).compute()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_11 = scaled_t_icombined.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_11 = PCA(n_components=2)\n",
    "to_plot_11=pca_11.fit(dX_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_11.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_11.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_icombined_SS_PCA.txt\", to_plot_11.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_plot_11=np.genfromtxt(\"Preprocessed_data/combination/Kmers_icombined_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onlyaMetagenomes))\n",
    "print(len(onlyaSGenomes))\n",
    "print(len(onlymMetagenomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=80000\n",
    "y=105\n",
    "z=22000\n",
    "\n",
    "#Make list of original indeces\n",
    "informative_kmers=list(set.union(onlyaMetagenomes,onlyaSGenomes,onlymMetagenomes))\n",
    "informative_kmers.sort()\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_11.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_11.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_11[0,aM_corrected], y=to_plot_11[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_11[0,aS_corrected], y=to_plot_11[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_11[0,mM_corrected], y=to_plot_11[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Informed selection of kmers from combined dataset every 50th',title_y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of an informed subset taken from the combined dataset with every 50th k-mer. This informed subset was obtained by selecting the k-mers that were present only in one of the classes and not the other. Example: If a k-mer was present in at least one of the aSGenomes AND it was not present in any of the other samples from the other classes, this k-mer was included here. The idea behind this was to **select the k-mers that were unique to each class and that could help differentiate better between them**. \n",
    "\n",
    "I only plot a subsample of the total number of kmers(0.1% of the informed subset)\n",
    "The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "combined=combined.drop(\"Kmer\",axis=1).values\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(combined)\n",
    "scaled_combined=scaler.transform(combined)\n",
    "scaled_t_combined=scaled_combined.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_combined.mean(axis=1).compute()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_9 = scaled_t_combined.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_9 = PCA(n_components=2)\n",
    "to_plot_9=pca_9.fit(dX_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_SS_PCA.txt\", to_plot_9.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_plot_9.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_9.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_9=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onlyaMetagenomestagenomes))\n",
    "ponlyaSGenomes(onlyaSGenomes))\n",
    "print(len(onlymMetagenomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "x=8000\n",
    "y=100\n",
    "z=2200\n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aM], y=to_plot_9[1,aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aS], y=to_plot_9[1,aS],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,mM], y=to_plot_9[1,mM],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Combined dataset every 50th kmer',title_y=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of the entire combined dataset with every 50th k-mer.\n",
    "\n",
    "I only plot a subsample of the total number of kmers(0.023% of the combined dataset). The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "x=8000\n",
    "y=100\n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aM], y=to_plot_9[1,aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aS], y=to_plot_9[1,aS],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  '<br>Dataset: Combined dataset every 50th kmer',title_y=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of the entire combined dataset with every 50th k-mer.\n",
    "\n",
    "I only plot a subsample of the total number of kmers(0.1% of the combined dataset) and they are either labelled as aMetagenomes or aSGenomes. The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome. **Here the idea is to see where do the k-mers belonging to aMetagenomes fall with respect to the k-mers belonging to aSGenomes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA on  arbitrary subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined=scaled_combined.compute_chunk_sizes()\n",
    "scaled_combined_subset=scaled_combined[0::100].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset=scaled_combined_subset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset.mean(axis=0)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset.std(axis=0)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='linear',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed = transformer.fit_transform(scaled_combined_subset)\n",
    "    X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This subset is 1% of the original combined dataset where we take 1 out of every 50 kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA.txt\", X_transformed, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces=classification.iloc[0::100].index.values.tolist()\n",
    "print(len(set(indeces).intersection(set(onlyaMetagenomes))))\n",
    "print(len(set(indeces).intersection(set(onlyaSGenomes))))\n",
    "print(len(set(indeces).intersection(set(onlymMetagenomes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=8000\n",
    "y=0\n",
    "z=2000\n",
    "\n",
    "#Make list of original indeces\n",
    "indeces=classification.iloc[0::100].index.values.tolist()\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_transformed[aM_corrected,0], y=X_transformed[aM_corrected,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed[aS_corrected,0], y=X_transformed[aS_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed[mM_corrected,0], y=X_transformed[mM_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Standard scaling + KernelPCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: 1% of the original combined dataset',title_y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior Kernel PCA of a randomly selected subset of k-mers taken from the  combined dataset with every 50th k-mer. This subset represents 1% of k-mers on the original dataset.\n",
    "\n",
    "I only plot a subsample of the total number of kmers(7.1% % of the randomly selected subset of k-mers) and they are either labelled as aMetagenomes or mMetagenomes. The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome. **Here the idea is to see wether a non-linear dimensionality reduction method could better separate the classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA on informed subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I take only the kmers that are unique to each dataset (aSGenome, aMetagenome and mMetagenome) but also that are present in >40% of the samples from each dataset respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=classification[(classification.Perc_aMetagenome>40)&\\\n",
    "               (classification.Perc_aSGenome==0)&(classification.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "b=classification[(classification.Perc_aMetagenome==0)&\\\n",
    "               (classification.Perc_aSGenome>40)&(classification.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "c=classification[(classification.Perc_aMetagenome==0)&\\\n",
    "               (classification.Perc_aSGenome==0)&(classification.Perc_mMetagenome>40)].index.values.tolist()\n",
    "informative_kmers=a+b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers=all_kmers.iloc[informative_kmers]\n",
    "\n",
    "icombined=combined[combined[\"Kmer\"].isin(ikmers[\"Kmer\"])]\n",
    "\n",
    "icombined=icombined.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset_1=icombined.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='linear',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed_1 = transformer.fit_transform(scaled_combined_subset_1)\n",
    "    X_transformed_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA_1.txt\", X_transformed_1, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_1=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA_1.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=14\n",
    "y=100\n",
    "z=1400\n",
    "\n",
    "informative_kmers=a+b+c\n",
    "#Make list of original indeces\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed_1.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed_1.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_transformed_1[aM_corrected,0], y=X_transformed_1[aM_corrected,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=1,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed_1[aS_corrected,0], y=X_transformed_1[aS_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed_1[mM_corrected,0], y=X_transformed_1[mM_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + KernelPCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Informed selection of kmers from combined dataset every 50th',title_y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is this plot?***\n",
    "\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of an informed subset taken from the combined dataset with every 50th k-mer. This informed subset was obtained this time by ***selecting the k-mers that were present in at least 40% of the samples in one class and that were not present in any sample in the other classes***. Example: If a k-mer was present in at least 40% of the aSGenomes samples AND it was not present in any of the other samples from the other classes, this k-mer was included here. The idea behind this was to select the k-mers that were unique to each class and that could help differentiate better between them.\n",
    "\n",
    "\n",
    "I only plot a subsample of the total number of kmers(1% of the informed subset) The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset (412GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in this part of the code is to see if the composition of the original dataset varies a lot with respect to the composition of the combined dataset with every 50th k-mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aSGenomes_HS_Oral=pd.read_csv(\"Metadata/aSGenomes/aSGenomes_HS_Oral_SRR.txt\",header=None,names=[\"SRR_accession\"])\n",
    "SRR_mMetagenomes_HS_Oral=pd.read_csv(\"Metadata/mMetagenomes/Oral_HS_mMetagenomes_SRR.txt\",\\\n",
    "                                     header=None,names=[\"SRR_accession\"])\n",
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "SRR_HS_Oral_aSGenomes=list(SRR_aSGenomes_HS_Oral.SRR_accession)\n",
    "SRR_HS_Oral_mMetagenomes=list(SRR_mMetagenomes_HS_Oral.SRR_accession)\n",
    "names=SRR_HS_Oral+SRR_HS_Oral_aSGenomes+SRR_HS_Oral_mMetagenomes\n",
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "total_aMetagenomes_1=(combined_total[SRR_HS_Oral_aMetagenomes] != 0).astype(int).sum(axis=1).compute()\n",
    "\n",
    "total_aSGenomes_1=(combined_total[SRR_HS_Oral_aSGenomes] != 0).astype(int).sum(axis=1).compute()\n",
    "\n",
    "total_mMetagenomes_1=(combined_total[SRR_HS_Oral_mMetagenomes] != 0).astype(int).sum(axis=1).compute()\n",
    "\n",
    "color_aMetagenomes_1=(total_aMetagenomes_1/n_aMetagenomes)*100\n",
    "color_aSGenomes_1=(total_aSGenomes_1/n_aSGenomes)*100\n",
    "color_mMetagenomes_1=(total_mMetagenomes_1/n_mMetagenomes)*100\n",
    "\n",
    "classification_1=pd.DataFrame()\n",
    "classification_1[\"Perc_aMetagenome\"]=color_aMetagenomes_1\n",
    "classification_1[\"Perc_aSGenome\"]=color_aSGenomes_1\n",
    "classification_1[\"Perc_mMetagenome\"]=color_mMetagenomes_1\n",
    "\n",
    "classification_1=classification_1.reset_index()\n",
    "classification_1=classification_1.drop(\"index\",axis=1)\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "classification_1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1.to_csv(\"Preprocessed_data/combination/classification_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1=pd.read_csv(\"Preprocessed_data/combination/classification_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMetagenomes_1=set(classification_1[(classification_1.Perc_mMetagenome > float(0))].index.values.tolist())\n",
    "aMetagenomes_1=set(classification_1[(classification_1.Perc_aMetagenome > float(0))].index.values.tolist())\n",
    "aSGenomes_1=set(classification_1[(classification_1.Perc_aSGenome > float(0))].index.values.tolist())\n",
    "intersection_1=set.intersection(aMetagenomes_1,mMetagenomes_1,aSGenomes_1)\n",
    "aMetagenomes_aSGenomes_1=aMetagenomes_1.intersection(aSGenomes_1)\n",
    "aMetagenomes_mMetagenomes_1=aMetagenomes_1.intersection(mMetagenomes_1)\n",
    "mMetagenomes_aSGenomes_1=mMetagenomes_1.intersection(aSGenomes_1)\n",
    "onlymMetagenomes_1=mMetagenomes_1-mMetagenomes_aSGenomes_1-aMetagenomes_mMetagenomes_1\n",
    "onlyaSGenomes_1=aSGenomes_1-mMetagenomes_aSGenomes_1-aMetagenomes_aSGenomes_1\n",
    "onlyaMetagenomes_1=aMetagenomes_1-aMetagenomes_aSGenomes_1-aMetagenomes_mMetagenomes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(701173468, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701173468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mMetagenomes_aSGenomes_1)+len(aMetagenomes_aSGenomes_1)+len(aMetagenomes_mMetagenomes_1)-\\\n",
    "2*len(intersection_1)+len(onlyaSGenomes_1)+len(onlymMetagenomes_1)+len(onlyaMetagenomes_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1=[]\n",
    "labels_1.append(str(round((len(onlyaMetagenomes_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(aMetagenomes_aSGenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(aMetagenomes_mMetagenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(onlymMetagenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(onlyaSGenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(mMetagenomes_aSGenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(intersection_1)/classification_1.shape[0])*100,3))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition of the combined dataset of every 50th k-mers based on assigned labels as aMetagenomes, mMetagenomes and aSGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVUAAADqCAYAAAAS59Y6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6eElEQVR4nO3dd3xb1fn48c8jyZZXhh0nzt57kJAQkkATwl6FFMpqoSGhtIwvFH6lBb6UYkwL37a0QAu0FEoZDaXsWUYIJBAygJAdsveOHceOp2RJ5/fHvU4U4yHbsq8kP+/XSy/LuuvRuM8995xzzxVjDEoppaLD5XQASimVSDSpKqVUFGlSVUqpKNKkqpRSUaRJVSmlokiTqlJKRZEmVaWUiiJNqkopFUWaVJVSKoo0qSqlVBRpUlVKqSjSpKqUUlGkSVUppaKoWUlVRGaIiBGRh2u8Ps1+/dkI1zNPRK5tTixKqdqJSF97f1xW4/VsEfGLyLYI13OviMxqkSATSDRKqpuBy0TEE/ba1cCGKKxbKRU9aSIyMuz/HwJbnQomUUWUVEXkThHZLCIlIvKNiFwUNnkfsAo42543CzgJeLvGOiaKyEIRKRKRFSIy1X79fmAy8JiIlIrIY/brfxaRnSJyWES+FpHJYetKFZHnROSQiKwVkdtFZFfY9O4i8pqI5IvIVhH5Wdi0e0XkZRF53n4/a0TkhLDpw+ySc5E97cKwac+KyF9F5H071gUi0lVEHrFjWScix0cYx4kissR+f/tF5KFIvgulqonINhH5pYisFJEyEXlaRHLs32eJiMwRkcywRf6FVeCpNh14vsY6a/3Nisg5wF3A5fZvf4X9+kx7HywRkS0icl2N9d0uIntFZI+IXGuXmAfa07wi8kcR2WHvA0+ISKo9baqI7BKR20TkgL2OmWHr7WDvw/kisl1E7hYRlz1thr1vPmzvx1tE5CT79Z32+q4OW1d9cWSLyLv2egpFZH71dupkjGnwAVwKdMdKwpcDZUA3YAbwOdYR7yV73huBvwO/BZ61X+sBHATOs9dxpv1/Z3v6PODaGtu8CugEeIDbsJJ3ij3td8CnQCbQE1gJ7LKnuYCvgXuAZKA/sAU4255+L1Bpx+IG/g9YbE9LAjbZP55k4DSgBBhiT38WKADGASnAJ1hH+un2un4LzI0wjkXAj+znGcDESL4Lfeij+gFsAxYDOfY+dgBYChwf9vvMBfoCxv670/6tDgfWAWcA2+z1RbLvzKoRw/nAAECAU4ByYKw97Rx7vx0BpAGz7DgG2tMfxip8ZQHtgHeA/7OnTQUCwH32fnmeve5Me/rzwFv2cn2xzox/bE+bYS87M2y/3AE8DniBs+z9OiOCOP4PeMKOIQmrACj1fi9N/DKXA9M4mlRTgf1AB/tLPpljk+odwL9qrOND4Gr7+TxqJNVatnkIGG0/P/JF2/9fy9GkOgHYUWPZ/wWeCfthzAmbNhyosJ9Ptn8ErrDpLwL32s+fBZ4Km3YzsDbs/1FAUYRxfAbkAdlO75z6iM8HVlK9Muz/14C/hf1/M/AmR5OqB5iDdVb5O+BXHJtUI9l3ZjUQ05vALfbzf1YnJ/v/gXYcA7GScBkwIGz6JGCr/XwqUAF4wqYfACZiJUo/MDxs2nXAPPv5DGBj2LRR9nZzwl47CIyJII77sJL3wEi/l/B60DqJyHTg5/aXA1bJKhsIAhhjKkTkv8DdQCdjzAIROTdsFX2AS0XkgrDXkoC59WzzF8CPsUrIBmhvbxP7tZ1hs4c/7wN0F5GisNfcwPyw//eFPS8HUsSqE+4O7DTGhMKmb8cqBVTbH/a8opb/MyKM48dYX9g6EdkK5Blj3kWpxon091jteaykcxJWIWJw2LRI9p1j2Pt5rr0eF1aJdJU9uTuwJGz28P20sz3v1yJyZHX29qodNMYEwv4v52juScLaN6s1tJ9ijKnts2kojgexDiaz7elPGmN+Rz0aTKoi0gd4CjgdWGSMCYrIcnvD4Z7HOt3Iq2U1O7FKqj+pYzPH3ChLrPrT2+1trjHGhETkUNg292Kd9n9j/9+rxra2GmMGNfTearEH6CUirrDE2pumNbrVG4cxZiPwA7t+5mLgVRHpZIwpa8K2lIrUa8BjwNfGmB0iEp5UG9p3au6nXnt904G3jDFVIvIm395Pq4XvpwVYiW2EMWZ3I99DAVCFdRCozgG9gcaup8E4jDElWNWPt4nVyPeJiHxljPm4rhVG0lCVjvVh5oNVMQ2MrGW+T7HqSh+tZdos4AIROVtE3CKSYldEV3/g+7Hqb6q1w6oTyQc8InIPVkm12svA/4pIpoj0AG4Km/YlUCIid4jVoOUWkZEiMj6C9/oF1tHwdhFJEqsx7QLgPxEsW1O9cYjIVSLS2U7eRfYyobpWplQ02Aft07CqzGpqaN/ZD/QNa6hJxqqjzAcCdqn1rLD1vQzMFKvxNw34dVgcIazC2sMi0gVARHqIyNkRvIegve77RaSdXfD7OVaeaZSG4hCR74rIQLGKqcVYZ+f17qcNJlVjzDfAn7AaVvZj1U8sqGU+Y4z52BhTWMu0nVh1sHdhfQE7gV+Gbf/PwCVitaD/Bau+9QOsEuJ2rIal8FOH+4BdWI1Ec4BXAZ+9rSDwXaz6kq1YR6J/YNX3NvRe/VhJ9Fx7ub8C040x6xpatpZ1NRTHOcAaESm13/8VxpiKxm6nyURciCQjkopIBiLtEElute3HMckTkTxJkTxpL3mSbj/3SJ7UPHuLScaYJcaYzbW83tBv9hX770ERWWqX4n6GleAOYTVYvx22vveBv2BV823Cam8Be1/FamvZBCwWkcNY+/KQCN/GzVh1oVuw2nX+jVWH2xT1xTHI/r8UKwf+1RhTZ7Ul2K1Y8U5EbsBKSqc4HUtMEHFj7QiZ9qM9Vv1RO6yShZtvV99UC2EdxCrsv5VYR+gCoABjSls09hhgJ8dMrLq7bKzPLiXs4aX+zy+IdcZTYj+Ksc5GDplcU9KSsccyERkGrAa8NepKE0pcJlUR6YZVXbAI60jyX+AxY8wjTsblGOt0rAtW/VV3+3lLXYJcSXWCtc5cdhPnO4jkSTrWZ9cZK4lmEUF7QxP5sOoadwO7Ta4paqHtxASx+rS/h9UY9BwQMsZ8z9GgWli8JtU+WIm0H1YJ4D/A/9qn722DSDus998Dq89wSyWBhgSxksR2YCvGlDsUR6NInmRjfX69sfpDO6UMK8HuAraZ3Pg+QNUkIh9gdVEKYrW73GiM2etsVC0rLpNqm2Wd1vcFhnJs95FYYbAS7GZgY6yVYCVP2mF9dv2JoI7dAVVYn906k2sOOB2MahpNqvHAuvR3KFZVh9fhaCLlA9YCq50uvUqe5ADHYR2Q4qIxCavhZx2w0eSaSqeDUZHTpBrLRDoD4zm2r1+8CWGVvlZizMHW2qjd2NQPK5l2aa3ttoAg1sFpqSbX+KBJNRZZg2CM5+gVbIliF7CYWrrdRZPkyUCsz69dS26nlVVhXam00uS2obaDOKRJNZZYjU8ncPTa6ERkgDXAEqLcsCh50glr3Imu0VxvjPFhjb2xJtEatRKFJtVYYHWJOt5+tJW7MVRiXcGznmb+CCVPvFgl02Ek7sGoplLgc5NrdjgdiDqWJlWnWZfGTcHqG9kW5QPzMaagKQtLngwFTsTqlN8WbQIWan1r7NCk6hTrWuKx9qOtlK7qEgQWYV0SHRHJkySsg9GAFosqfpQD80yu2dXgnKrFaVJ1glV3ejrx3SrdErYAnzVU12rXnZ5BbPY1ddJqYLHJNTowj4M0qbY2kRysQYLb6ulqQw4DH9XV/UryZBjWWKDu2qYr9gKzTa7xNTinahGaVFuTyACsEc01IdQvCHyOMeurX5A8cWPdrmOgY1HFj2LgfZNrDjsdSFukSbW1WDcEjGRMV3XUEoxZKnmSjDVOZ3enA4ojlVgl1n0NzqmiSpNqS7O6S03h2NtWqAgdymBV1i/oxtFb6ajIBYFPTa7Z5HQgbUlb6RPpDKuF/ww0oTZJZSqBr09nyJ27SXM6ljjlBk6TPIl04GcVBZpUW9YpJN6lpq2iMpXA/LMIVaaRfHIpaXftplwMelrVNFMkT/o6HURboUm1pYhMQkuoTeLzEpx/FiFfGkdu7zKplLQ79tB6t5tJLAKcLnkSi8NFJhxNqi1BZCzWvbxUI4UE88VUqsITarWTS0m79CBxMQh2DHIDZ0meaN/oFqZJNdpEhmMNiqKaYOUEKg5n1d2H98oCUseVopdkNk0ScK7kSabTgSQyTarRJNIVa5Qk1QRbB1G+q1/9jVJukDv3kNTNT1VrxZVgvMCZkidO3X4n4WlSjRYRL9b91Nv6dfxNcrAzlWvGkhrJvCkG9292YlJC9d9/XdWpIzDZ6SASlSbV6JmKdStj1UiVqQS+moIHV+QHpJwAyXfvRi/FbLpBkifakNoCNKlGg8gooI/TYcSrL6YSCCQ3/m6wo8tJ/UEBZS0RUxvxHcmTjk4HkWg0qTaXSDYwwekw4tXWwZSXdGz64DKXHiS1c5XWrzaRBzjDHldBRYkm1eawLkE9Df0cm8TnJbhudPPuDpsErv+3F72tSNNlAeOcDiKRaDJonlFYlf6qCVaeiC/oaf6IXaMqSD25RC8MaIbjtBogejSpNpVIOtao/aoJ8nOo3N8zetf037Afj1d7AzSVC/iO00EkCk2qTXciVmdq1UghwayYEN3fXocgSdfka2m1Gbrr+ADRoUm1KazGqUFOhxGvNoyivDL925ehNtfZRaT18RHV2163MSdKnmhOaCb9AJtGW/ubKOAmtHVwy9xKxg0yPV8brZqhIzDU6SDinSbVxhLpDOhoP020fRCVwaSWu53MuDJSs6s0sTbDaMkTvSqwGTSpNp6OPtUMW4Y0vpN/Y7hBLi3UK62aoR16IUuzaFJtDJE0oL/TYcSrPb2oqG1Iv2g7vZgUHRegWbTg0AyaVBtnOPqZNdmmEa0z2IzX4L7gkA4P2AzdJE86OR1EvNIEESkRN1ZSVU1wqBO+w5kt00BVmwsOkaS3X2kWLa02kY6pGLkBUHtSaA8PJEOlC4wLgvvggVnQ8w640gfeLDj4ITzdj2NLT59B5o9gZim0BzgTPvsPfAIwEn5yALoCVEJqClQcgN/MgU7nQF4W7AfoB1u+gBcOgecEuLEYMs+BebPgU4AT4aob4LOZsKMFP5sGbRhJsDW3lxkk6TslVMxvH9lwgi3m71xNPqNIooQ7yDvy+qucyiamIhhyWMUMXot42XzSeIGfUkknUjjIVTxJNuXMYTRLmQYYhCCTeZmJbGIdObzDtRjcTGUWJ7KFKlw8zi1cy+Nk1NoNbYDkyWKTa7TE30iaVCM3sL6Jc+GhUVBa/f8vYfov4JXbYOO1cNL1cNaH8Hb4Ml4I3QuvzoQd28E7Bu5+BdZeCntXw1PV850Cl2RwtGN7R8g/AL8JX9eDMHwYbHoD3u8LtwOfzoKeIXA5nVADbkIFOa1XSq12ZjFmfvvW3moNx7GQFOYym5lHXlvAEHYxhlv4DakE2Eu7iJcF+C/nksM6fsAHvMg5vMs5zOB1xrOO01iBC1hJD97jp0wkl0VMYTIv0Y0C3uEKTuQJ3uIUBrC4joQK1u1X+gFro/I5tCF6+h8JkWSge2MWKYQut8JGgOmwdkktl7ROgOLqhNcHfF1g74YaYwkEgaVwwg3wVX3bS4ZgJSSXgtvYA2XfB9MehbcaE3dL2N8Tn3G3/m9tZDkpjl+6OomNdKgxPOFyTmEc75Nqd/3qRknEywLsYzSTWQTAZBaxjzEAdMB35FOuxHukBlsI4iOZSpJxEeQgqeziOM5jcQPRay+AJtCkGpme1PNZCXAG3JIDv5phj6ieDXt/hfVjfxTGlVijAdVpDnTaA71+CFvDX/8zDEqHw9+FA9WvFUN2F7i7N/ziYbsEfTusPQCdhsCd34dP7oLjBsCOSVDc1DcdLbsd2jWTwHViaQx2ryojh+0M4o/cyUP8gq8bmbyqaE9P+3vtTjFVHC2Pf8QYfkceH3Izp/IcAFOZx1LO5V1m8h3e423OZzzv426wzrmH3nal8fQDi0zf+ia+B384GYqWQbuz4NaHYN9j8NxtcPkzcP44WOGi7g7pO8F7JVx/Pbxcs971ZRj/nbBS6jgoXg53joCyp6H3rXDj9+DeflC5Ep4GKAH3KLjlU3h8ClxaAFnfg0UPwMpmfAZNYsAU5LR8N6q6TCmJgSqAmgwufKTzc37HUvoym+s4nruaVMRx2WusdibLOZPlLGIQC5nGiTxMXwr5f/wJgA10ppxM+rGXv3ENIdycwVsMOXrQDuMGelHjQK/qpyXVhlhjpvaqb5aToQjgeCg5AZYvgL4Xwb4t8Of9cP/18GUm5Ne2bAm4T4Hrp8AXD8Ky8Gnl4FoJY28JS6qZEBiBdUr4Y9iRCfkfQ074cj+BU86CRbOgfwZULIUnZ8FZTfsAmqewM75QFIb3a6qR5c4l9DqlcIihLMMFnMA2IMSBRtyKJ4nD7KIDALvoQFIt1QeT2EgF2eyvsd6P+B5n8BZzOJ3RzOc8XuMTLqhna1oF0EhaUm1YV6h7IOU9kFwF0gd8eyB5JQz/Kby7DNodDyVVIPfA+efbrfHhgsBkmN4D9r4Cc2pOfxCGZcO+yXbSBlgBGUOgLAXMh5BdCF1ODkvY6yHtCzhuA/z5HjjOBcYNVLXSiFrj4Oq1MCoNSgogb19PQnO30vPZFVwZCOLN8HLwvqk8nZNxbIn8sA/PTe/xy6DBYwzuQZ34Om8q7wC8+g1D3lzHJSGDp3Ma2/9wJs97PYSeW87o2VuYJmBECF46nJcvHMKmxbvIefwrrg0Z3D8cyax+vVi31U1KA63drac3y9nCEE5mPevpgsFDl6ONnA3qygrmM4kf8AHzmURXVgBWKXQg+biAZfQmhIfOYetdyCBSKWYIB1hAMoJBMATqPfD0ljwRk2u0e1qExOhnVT+RE6hn3NQPIfsquAEgBO6T4It34P0r4LQP4VSshZfOhjfcwCLo8COYvgke/TMMvBV+mQ27xT6FuwHeyIPV9nIzRsKW5+Gz6u39Ao5/Bqa5ISgQuhbeuT/stH4KXHYBLP8lbCgAz4nwP8WQeTZ8+m+Y2yKfUZiHYFAm+H4JMwsgb955+C76hLyLh/LKRcPY+OgXnJRfTvZ9px7bEyJkoLACb3Yavooq3Ne9yy+vGMlLZw1g2w9e4/9+MYmHJvTkwD1zuTA7jYM/m8CCgnK8Wan4XALzttHj71/z0xe/T+6dc7j0pF4sG5BFwWNfcsXk6fzpxXl8Fy+VXGA38LSWv3IthxhMgAw8lDCStzmbxTzD1RymFy4CTOJVvsN6dtKB15nOLTxa57LTWMAB0nmBn+IjixQKuZK/05ly/sPZbGcSQhA3fk7mNSayyfqAgb9wK1fyJJ0p5xu68l9+jMHNKbzABDbX8y5eNbmmsOU/rMSgJdWG1XtlydlQkF+jexOA3d/0k5qvT4LiTVg7zS2w6Ra4rq51L4Vna772R1j2xxrVBOE+g5ern2dDYAv8ub74o+3nsHGO/ZmFBFPajuRSP12mDbV6Qpzaj7X3z+cWanQvcwlkp1mNSpUB3CGDW4CdxaS7hMCEnlad37hufPPuRs4FFlTPD1DmP3o24RaCFVUkl/pIdgnB7L2ks4vjuJm/tPgHUNON/KPW12/gn996rRfFRxJqfct2oYz/x8Pfev0KPgQ+rHUZF3Arjxz5fzj7GM79dQd+jGxAk2qENKk2TC/Xa6KydlThIrm9l73Pr2DMjDEsf3cD4yqqau8J4Q8i17zF3aV+Oo/ozLxzB7E1ZMAY3LM30+esAWxfsJNx5X4yq5d5ZjljZm/mIl+A9j8+3kpIV4xk3kOLmRkyeGaOYdZbn3FxhK3dqna6DzSCJtX6iHihEQ0I6hiHO1pXUV0/jueeXsblH2/l/AGZrHBJ7T0hkt2YWRfzmz0lpN79CTd8tp3uU/qw5+rRPPXvVVw2ayWePh35RuRocpw5huUzx7D8zXUMen0d084fzMOjcih8ZprV2v3VHjr7/HSiL3siaO1WtdOk2giaVOunP6ZmKOlodbyf1It9k3pZ1RBf7KLLlkP1X1fevR0VfTqwfsEORkzpw57zB7Pl/ME8CPDSaoYXlh/b2wHge0PZ+OJqsrcVkdG349HGmWeX870Zo3nr97M51T+a+XTjIB9wEUOs7mcqIrofNIJ2qapfttMBxLPDHa1rejYfsi7DDISQF1Zx/vju3+4JsfUQGXtKrOv0iytJ2lrE8J4d2Be+fKkfz+wtnH1qP2v5r/bQOWSXWedsoXcohKd3h6MJ9Y21DMpIpvjEHhzw+kmKsLVbfZtX8kTP2CKkJdX6ZTY8iwo3Cq7dAoMrIeOUBfxlch/eqQzgXbrX6gkxIIul/3MiCwHWFdDhT4uY/tQFPLq9mA7/WMpMAy5jkCGdWPKj41gF8I+lnLWtiOOMQY7vxqeXDGc9wOxNjH14EZNcQtDtwv+j0Tzlsi/NDBl4dyPn33sKTwKMHsnHn3/OTUdau1VjZUEjun21Ydqlqj4i59JAx39Vt/cuIxRy4Jr/2izOoPz+HtG7JXYb9JnJNeucDiIexMQPPoY5O2xcHKtMJRArCRWguz92YolTui9ESH9o9dOSTRMFY6z7UrJpnbsOJDBNqhHSpFo/bdBoolhLqh5Nqs1V56Xa6liaVOsiImhDXpOFNKkmmlYZOyIRaFKtm/6ImkFLqglHz9oipEm1broTNkPIscH+aqdJVbUWTap1q3NQadWwYIwlVbcm1ebS/SFCmlTrYkwQYusUNp7E2um/W888mkuTaoQ0qdZPf0hN5ImxTy6oB8jmirFvNHZpUq2f/pCaKKXcuVuo1OawW7/LZtLPL0KaVOunP6QmSqmIraRa5NGSajPpvhAhTar1c/ZeRnHM68MtIWvov1hwyGON7aqaTPeFCGlSrd9hpwOIZ0n+2ElkB/UyjubSfSFCmlTrV+R0APHMWxk7STVfT/+bq8jpAOKFJtX6FTkdQDzzVsbO6f+BJO1S1UxFTgcQLzSp1q/Y6QDiWWpZ7JQODyTpb70Zykyu0YaqCOkPrX5FTgcQzzILml46DBkhEPKEqoLJwapgcjAYcje51BsCsylFr11vhiKnA4gnWn1fH2P8iJSj46o2SacDdQ9K4wukBEv9HatK/JmhUn9HU+rPdJVVtXNVBtI9IeN2gQi1HvRDJLmqAimeskB6cpHJSC4OtUs+5GqXfMidnlzs8bgC31om30OVz6VJtRmKnA4gnmhSbdgBoK/TQcSj9FKS3FUE/W63FFR09+8r7RssrOjqqajK8ARNkhua0pfVRVXI66nyez0l/qxvTU12V1SlJx0OdE7bFeqasS2pQ8rB5E0pBNBRlppDb+fdCJpUG7YLTaqNVklWYDen+NeszQhsSknOMLhTWmO7/mBqkj+YmnSoMocNheNIcvkCC/smVdF+bQXpX3pxVWmVV+PtdjqAeKJJtWG7nA4gXgTwhrZzXuUuTnOV0NsLrjTXwQ1lpsd6xxJZVcjr2WhOT2PPtCTEHyJ1dQUd34N2X+jtQSJTaHJNudNBxBNNqg0x5jAiJWDde159WwXZgU1c6tvFaSlBUo6pf84pzkla32O9U6FR5vb6S5PSrFN/k+yifGwq5WPBc8BP1htBOnyopdf6aaGikTSpRmYXMMzpIGJNEQN8G/hh8ADjUsGdXts8HSo6JHuCnkDAHXDkt7Y3rVPt9amBLskcuA4KrgrQ4cMKsl5LwXM4psYriBGaVBtJk2pkNKmGyef4yrVczWEGRFRP2qmkk39/x/2O/Na2ZnSrv1tXKN3DoYs9HLogRLvF5XT+ZzJJBbpfWILAXqeDiDdiTMz0z45dIsnAj2hSa3Xi8NExsJxb/PmcEHEXsxd5ccQbvHFZ0B10d8vo9vnZA8/+oLb5Fu9afPzK/Suvn9JnygNDs4du3314d6f3Nr6X5/V49wO0S2635aJhF70A4A/63f/d8N8fFPuKhwgSGpI95M2JPScu+3jLx6fuOrxritfjLbx42MV/NUnpvucDlUPMrkXjOOP3L0cUsPhDZL1cQaeX0xDT1q/C2mFyTa3fl6qbHpEjYfVX3QYMcDoUJxjEbOF7Feu50hvCG3FC9eOXN3nzB3dy5yN7huzZ/9yG5+7dcmjLiv6Z/Y8p/ZT4SrybCzefnpaUtjX89WR3cv700dN/U3O9c7bMOS/ZnVwyY8yMX4dMSIoqi9IB9pbunfCj0T+6b/bm2eeu2LdiRMrw7y82Xzz6XU7JfSryN5vs4uBV6Rw+w0fXhyBtbVu+NfMGpwOIR1pBHznnWlscVMQg3zz+5l/LNWkhvI0qqX/ER/3a0S5/LGMLBhcOrsxJz/lq48GNo2vON3fb3GlDsod8KEhVJOvdV7rv5Kl9p74P4BKXyUrNKq2eVhWscgdDwWSXuIJLts6dQqdBq+nQq/Gt11Vdvez8g5c9t5cTzIiZgWFakQ/Y5nQQ8UhLqpHbDZQCGU4H0hqCJIVWcUPFLk5PA1eTToP3sa9jO9oVAvQp6JOS1jGt8GDFwWNK++sL1veuDFRmndD9hFXrC9afFT7NH/RnP7/i+bvd4q48Lue4N0fljNpUXFmcCvDx1o+nFVUWDU7xpOSf2vfUFzundy7p06HP3JfWvHRnWlLanqzu41ZXrX3jNs57/M9N/QwAKJmcRum4IDlPlNNhblu6sm6jyTUxMyBOPNGkGiljDCJrgfFOh9LSKskKLOa3wVJ61dqi3xTegNed7Es+piQaMiFZsmfJpVP7Tn225vzZadnF3x/+/TuzUrPK1hWs671w58Ib+3bse2/QBN2BUCCzc1rnzRcOufCVjzZ/dMan2z+99JLhl/xzcp/JX0zuM/kLgH9tn38OfaZ8wjevjGDX4kl42x3itPtfweVpfCOCSXOz7+dpVIwoJ+fx1DZS1/qN0wHEKz39b5y1EDtjhLaEQob55vE4pfRqdl1iV7oWlVBy5FrSUHkoO9WTeqj6/1J/aUploLL77M2zb3tm2TMPlFeV91+4c+H/rCtY18fr8QayUrPKAIZmD93hdXvzd5fszslMySwVxD+h54RlACO7jPy6zF/WO3y7O8sKMioqiwYyZsZyts09i3MeeRJPajlrXxvarDdUfHYaO/5YSTAtoX8DwG6Ta4qcDiJeaVJtDGMqgU1Oh9FStnFu+SIeSAqQEZUzmDM4Y1sJJV2WsaxTGWXu7cHtE4e0H7Kkenp7b/uKH4/98W0zj59518zjZ96VlpS25aReJz0+NHvo9oPlBzOCoaAA7Czeme0L+rrkpOfkiwgdUzquXLl/5WCAjYUbh6Ympe4J3+6nuxZeyIjL3gEgFExCXCBiqKps/vX/lYNT2frXIL5eiXx7kVVOBxDP9PS/8b4GBpJA3asMYlbws4pdnBHVOkMv3tCFXPji7/n9rQYjIxix4Fz/uTv+tPFPV2SnZW87sceJK+tadmPhxkHrC9ZPE5EgEBrVZdQLmamZ5QAn9TrptXnb5l2zcv/Kyz0uT8nUvlOfq15uXcGGHpWeFBf9z9gBQJcRX/LG9FyS0wuZdNuHUXljwU7JbH8kRLeHKmi3INEud91ncs0Op4OIZ9pPtSlEJgLHOR1GNATwhhbzgK+Iwa2SHEISMnNGzqnyJftaZNSotR16l8/PGd16DUqdZpWR/VLU6p5jwFsm1+x3Ooh4pqf/TbOcBLi7ZJCkVk2oAC7jkuG7h7dInWSVuINfZg9r3X6lB69Kp+DyslbdZsvZrgm1+TSpNoVVt7rC6TCaI0hSaBG/a9WEWq1nYc/UjIqMymivd2XmgEqfO7n1q2UOXpXOwUvjfSQnA3zpdBCJQJNq060C4nJHMoj5gjxHEmq1MdvHCCZ697Aq9aT4l3Ua5Fw/0oLpaRw6Py5/D7aNJtccang21RBNqk1lTABY7HQYTfE1d1QUMsrRBpbMskxv90PdK6K1vs9yRodC0rSLFKLmwE9SKZkUtffUinxoKTVqNKk2hzGbgC1Oh9EYq/lJ2T5Ojokrg47bcZzXE/Q0+y6dO9K6lO9K79Iqdxaon1vY80sv5SOiXrXRwj7XgaijR5Nq880nTqoBdjO5YhsXxkxLdVIwyT1i54hmNfj5xR38rOvoGLr/VJKL3Xd7CLSPlwsENplcs9npIBKJJtXmMsYHfOp0GA2pIDuwkptiKPlYeh/sndbjYI8mHZQMmI+7j6sq96TEVn/rUIaH3b+Kh94hZcACp4NINJpUo8GYnViXsMYkg5ivuDsQJC0mL1gYs31MaruKdo0+ZV6WNah8Z3pODJz216JyeGoc9Aj41OQan9NBJJrYOsLHt0VAN6Cjw3F8yzqmlx9mQLofv1zDNb9KI63oSZ58LHyeO7jjst3sHgIQIJDsw9fuDd64tXr6fvan3MRNeX3pu/xBHnwRYCYzb6ugooMHTxVAHnmPDGBAyYM8eOoylk1JJ73wER75azrpwbd5e+ACFoz9Pd8eLNplXDJx40TPvOHzAlWeqoh+k7vSssuXZA+tvSpj1YsjWP/W5WBcdBn1OVPuPnagZX+phzl3zKS8oA9ubymTfv4UXcccZM1Lw9jw7sWEQm5criBDpr3K8EusIR/9ZW7m/voHlOwZgkiI/me+yfHXLGPhg6eyd9kUktMLOfuRv5KcHmT92wPZuWAsZ9z/H9KX+kjZHItjsq4yuUZvldICtKQaLVZvgPeBmGr9LWSYbzMXpQE8xEOnd6RjrbfH+D2/f3kWs34zi1m/Gce4T/rQZ1n49D/xp2k55Hxr0OJruObp6uUGMKAEYBWrJjzP8/f1otfm13l9RIgQb/P2+Tdx07t1xZlSleIZv3l8IJJuViWeVP9H3cfXXkIN+oX1b/6Ak2//Cxf+M5eCtePZ8Xm3Y+b5+u8n40kp55KX7qb3d+aw9KmLAUjNLmXyrx7j+y/cx9ifPMPql645sszCP5xHcnoJl/zn11z0r3vpd9pGAPavmsBFz99H+16bWfv6CEwI1r99PuNvehc8LnbfLYS8sTaE3g7itOdKPNCkGk3GlAAfAM1u0Y6GKtKCS7jLBW5Zw5qOG9k4aipTP29oudWsPnESk450sfmIj3qXUdZ+KEMjHg6ukkq3H3+yB0/wCZ6YMIhBq3tR/2DRnUo7pQzbPazeg1JAXMH3ek6kyuWp/be7+aN+JLfLp9vYApLTg2QP+4pt844dGDt/7Rj6nrYIgNFXL6Vk71BMCPqfvpPsocUA9P7OHkwwGX+pVXLO/+ZkJtz6PgAuj6Fj3yMDYxOodBP0J+PyBFnyxIRjBsYOZCez75ZY6g1QAMwxuXp9ekvRpBptxuQDH0P0OrY31Rp+6vPTMQngcR6//AqueM2Fq964VrIyq4yyThdx0TqAAAH5N/++9EZufKW2+Z/juauv4qpf55F3fgirQDaRiXOv5/o7iynOOp3TNy1hyck/42fzIol54P6BaTlFOXUm309zxviLkzPqbnAr3dcRrzUwNgCpWYfwFWceM09VeUcy+1rzeLwh3EkVFO84dvDxZc+MJTVzB8kZAUr2WH16Fz44jdev+hX/vfGnFG6yblnec+Jc3r3+TnzFWfQ/fRN7lpzMiTXea8nkNMpGx0JiLQU+MLkmJg76iUqTakswZjuw0MkQyuhWtYupqQD/4l+j0kgrOZMzGxx96B3eGd+PfkuTSTYAj/DIKQMZuGoEI4pqzns7tz/9Ai/c9xAP/WEnOwf+jb9NBLiRG7+YxazfPsqj//wbfztjClM+eYVXRtzIjdfdwR2XBQjU20l/3JZxKR1LO34rCX3ZaWj55vY9Wv6ihR3zu7Fl9sWccOMsAIJ+N4HKTLIGbebiWffTofcWvvjLpQCMv/ELLp71W8599J989bczjgyM/d8br2POHZcRst/rgeucHtjaD7yv/VFbnibVlmLMGqyBVxyxmusC4BaA9awfuJ3toy/jsgf+zb9/sp/9Q27m5mtqW24ta8efwilHTv23s33AcpafehmXPfAJn1yyiU0Tf8WvLgIYxrAigC508Y1hzJfb2NYvfF3rWNdhN7v7zWDG8rnMPesRHnkyldTy16h/sGi3cbtO2nBScmZp5pGqgC+yh5Uvj+Qy1IyuRfiODoxNRWEm3g7HXn6ZlFbEoW3WPAGfi2BVKh16W6fz+Ws68uXjNzLqqmfoPi4fgA69SxG3nzFXW/XMg7/7NeUFxwyMTcG6DpTs7lfnwNj+Xl6KpzpV3x4APtTLUFuHtv63JGO+RCQAnNCamy1ikC+fcUdKdL/lt28AbwC8zuuDP+CDsx7l0X/WXG4Ri7r68aedx3lHrhJ7lEefrn7+GI9N2s72vvdz/xs+fK697E3rS9/SCirc3/DNcQMZeEy3sid4YtplXPY2QJBgkgsXgphKGh4s2m3crkkbJnkXDV5U8X7frmZl1sDIrgLrf8Y2Vj7fhb3LOtFpcBEFa8dzwvX/OGae7KEr2PbJJAafv4UVz40lo9t6xAUle1KZ/8DNDDj7dYZccLRDvLigfY+VrHtzMMMvWc+2eUNJ6XjMwNgseWIaI6z3WufA2Pkz3LSfb5Bga5Zaq0uoOvpUK9Gk2tKMWYpIFTCptTa5ihsiqs+9h3suHMjAbdOZvhLgQz4cP4hBX7kiOIEppdTza359S4iQ22BcPem59iZuml89/WM+7gVwBtZg0SMY8eV0puemk154GxEOFm3csnLDT4Mru+W7ID+iRfB4Qwy58EUW/P5WMELnEQvoPXkvc++5kKyB2xg9fSXjfvo5c+68hlcv/y1ubxkTb7VuYb30H6fiL+vC1o+/y9aPvwvAqXmPkDmghHHXvcbih69h3RuX40kpYcKtRwbGZov1XhscGDvYKZlDF5aT9UZrXSZcCbxnck1BK21PoYNUtx6RgcBUWrjKZT/jK77inrgfjb4KCT3IWN8iulnvZeLycvrtiokxC5rFVRpgwAwXLl9LV70dxkqoh1t4O6oGrVNtLdbgK/+lhQe3XsO1cf+dluEJ/JqJ/iMJFWDxmDTWDCyL5nCBjghleCj4YUvXrR4A3tSE6oy43wHjijF7gdeAFqnfKmBUZTndY/HqnYito2PF9ZzKGjp9u3P/yqHpfDLRR+Wxt7qOO8VneTHSUgeHlcDbJtfEQheuNklP/50g4gLGAWOAqDVaLOGu8n1MistT5CCYFxlc/hKDGx5FK9kf5OSv/XQ9GL/VHN0fqKDdomjGXwHMM7lmZxTXqZpAk6qTRLoDpwLNHo4vQEpoNi8QIjnuzj4KSPE/wAlmIx0bV8oesqWc0WtTcJu4e8+kraig193RSqq7sBKq9kGNAdr67yRj9iDyGjAZ6NfQ7PXZyemVIZLjrpS6gK7lDzMmxUcdl53WZ33/NPZ29nPKl5BREXPDGtarfGQKgfZBPIebM3JYEFhick1c3y8t0WhJNVaI9MDqdpXV0Ky1+ZRHK0voG5vD4NViD2m+JxlpviYKI/ZLyDB8UznDNqeQFIzJ4Q1rlf2vcjq93NQD4SbgS5NrShucU7UqTaqxRESAoVgXC0R8alhKD/88noiLkloRyVXPM7TqI3pHv1SdVBVk9LpK+u9IjYsqAc9+PwOubez3dgBYaHLNgZYISTWfJtVYJJKM1Yg1Cmiw5LWKG8q2c17M3CalNuW4g68y0Pc6A1KDtPAN+lIrA4xd7afXvlQkeg2BLaL3z32kboykLrkUq2S6qaVDUs2jdaqxyBg/8CUiq4BhwHCgzpLdAcYltVZojVWOOziHXpX/YmhqJZ7WqfOtSPGw4AQP7Uv8jFsToEtBKq4YTa4lUwINJNUDwGpgi8k1sTYuq6qFllTjgdUFqx8wAugaPslHx8BH/CumDo4hMOvIrHyPvnxOt5QWL5k2JMUXYOA2H/13JpHe8LgDrSp5WyX9bq5ZrxwENgNrTK6J8PpcFSs0qcYbkWysetd+QOoOzixfyc9iotX/IF7/x/Sq+i99vYXE2M34qmUX+hi8NUiP/V48oRho1AoaBl1u7MtWD2Il03XaeT9+aVKNV1ajVs4fmHV8J7rm5OBMl6I9pPlW0ykwjx7uVWTHTe8D3IEQffZU0mM/dC5MxhvZvbGiKiQhDrX30/7lDYx8bI3JNSWtHoOKOk2qcU6ES4HMLCoDE9jnH8VBelDm6kpZUhrR7V5UhYR2k+HfQMfgSrJdy8hOPow3Bkp7UdC+xE+3/ACdCyGrKIn0yujWUxsMld4ApWkBituF2NPFxb5sL0GPC1hqDEuiuj3lGE2qcUyEZGBGXdPb4wv2oaSqLyWhvhw2PSl1eQlKEiE8hMRDSJIw4iEkbkISQkwx3uAhvOYgKaGDpJgDpEo+qa58Ul07aJfkeP1oa0mqCtKhJEhaRZC0SkN6hSGtwkWqD7w+D16/CyOGkMsQEkPIhfXcZT33J4Uoamcoai8cau/hcIaHkLuubl67jOG9Vn1/qsVoUo1jIvQAznc6DtVsfmN41ukgVHTEfgdpVZ8OTgegoiJZJPKLPVRs06Qa3zIankXFCf0uE4Qm1fjWzukAVNTod5kgNKnGN90RE4eWVBOEJtX4pjti4tADZILQpBqnRHBTz3gAKu7oATJBaFKNX/Fz9ZKKhLb+JwhNqvGrbXTCbzv0+0wQmlTjl+6EiUW/zwShSTV+6U6YWPT7TBCaVOOX7oSJRb/PBKFJNX7pTqhUDNKkqlRs0INkgtCkGr90ZPjEot9ngtCkGqeMoRLQG8EljjKnA1DRoUk1vumOmDjKnQ5ARYcm1fimSTVx6HeZIDSpxjct3SQOTaoJQpNqfNMdMXHoATJBaFKNb4edDkBFjX6XCUKTanwrcDoAFRXlxmhJNVFoUo1vBwG9HW7804NjAtGkGseMIQAccjoO1Wz5TgegokeTavzb53QAqtn2Oh2Aih5NqvFPd8j4FgIOOB2Eih5NqvFPk2p8y7ercVSC0KQa5+xWYy3pxK+tTgegokuTamLY5HQAqsm2OB2Aii5NqolhC9q1Kh7tM4ZSp4NQ0aVJNQHYVQDaCyD+bHY6ABV9mlQTh+6g8cWgp/4JSZNq4tiCDlodT/YYQ4XTQajo06SaIOw7AWhpNX6scToA1TI0qSaW5U4HoCJyyBi2OR2EahmaVBOIMRwC3VnjwHKnA1AtR5Nq4lnudACqXiVoNU1C06SaYIzhALDH6ThUnVYYow2KiUyTamJa6nQAqlblwHqng1AtS5NqAjKGPcRM3WreCMi6Dzr+Fr53zrenH/LAyJ9Y07vfCXM6HZ027Rzr9az74L7hR18fdzWk/RGyc49d12kXQ+d7YMzMo6/dOAEuPz2676nJFhtD0OkgVMvSpJq4FgBVzoZQKfDwD+DFv8D2XFgwHl7pduw8N58M6eVQdDdcOgduvdh6/ZVusHA8bL0X/vNneOiH1voArlwIj//l2PVsToWtvSH/PvAE4MUecCAJ3jsJ/jGvpd9pBHYbo2M0tAWaVBOUMZQBS5yN4ul+kJUPZxdAhyCc9BXMGn3sPAvHwPRF1vP/Wwqbh0IQa76TvoLMAJx10FrP0/2s+X6+EXrVuJOsJwRBt7WsPxlSgnDNWXDFXGjndOkwCHzucAyqlWhSTWyrcfT+R1s6Qlbh0f+7H4L8zGPnOdwRjrPnSQuBtwK+ybDm6xF2q5isQ9b66tLHByesgm6/hsxi6F4B6/vB75ZH5700yzJjKHY6CNU6PE4HoFqOMRgR5gPfA8ThcFrB67OB2dbzE34Ed70N13wHvhgO/XbBu+85EFQR2s2tTdGSaoIzhnxglTNb718EhVlH/9+TCZ1r3KiwfRGstOcpd4EvFYaXWvPtDivVFmZa64vEs70AgbP3wbxxsOZJ2NMZ3u3S9PfSJAb4TLtQtS2aVNuGL4H9rb/ZmdvgYBeY3QmK3VbD05Urjp1n0gp4fpL1/H/HQv/14Maab+F4q3fA7E7Wen4c4Sj590+DR9+CMjcY+zfuMlCUHK13FqGvjNEhGdsaMUbHNm4LREgHvg+ktO6Wc0fCXy4HIzB5AbzzPpxzIYzdBg+shAIPnHIN7O4NqWXw7FNWwxbABefC/JPBFYKfvQT32oOQjLoWtgyGygxILYHL34anF1jT7hwDK3rB++9Y/0+9BFYPh+67YeXTrfjGdxjDB624PRUjNKm2ISJ0B85Dz1BaWjHwhjH4nQ5EtT7dudoQ+6KARU7HkeD8wIeaUNsuTaptjDGswepqpaIvBMwxhiKnA1HO0aTaBhnDQmCt03EkmBAw2xh2OR2IcpYm1TbKGOajiTVaQsBHxrDD6UCU8zSptmF2Yl3ndBxxrvqUf7vTgajYoEm1jTOGz9Dh6JqqOqFuczoQFTu0S5UCQIQTgTFOxxFHfMDHWoeqatKkqo4QoT8wFR0ToiGFWI1Sh50ORMUeTarqGCJkAWcB7Z2OJUZtAeYZQ8DpQFRs0qSqvkUEL3A60NPpWGKIwbqWf7nTgajYpklV1UoEAUYDY9HqgMNYo03pDRVVgzSpqnqJ0B6YDPRwOhYHhICVwFI93VeR0qSqIiLCYGAirT7KlWMOYJVOCxucU6kwmlRVxERIwUqsg0jcOwn4gK+BNcagO4dqNE2qqtFEyATGAf2djiWK/Fh3SFilI0yp5tCkqprM7n41GhhA/F6dVwGswSqZ+pwORsU/Taqq2UTIAEYCA4E0h8OJVAHWuAfrjcHpW1irBKJJVUWN3Q2rO1Zy7Qe09j2hGlIMbAI26S2jVUvRpKpahAhuoBdW1UAPnOs1UATswEqkBQ7FoNoQTaqqVYjQAcgButqPji2wmSCQD+zDunvsfmOobIHtKFUnTarKEfalsFlAOpBhP9LD/rqxGr9cWJeIhuxHFVAKlNl/q5+XAIXGEGrVN6JUDZpUlVIqiuK1G4xSSsUkTapKKRVFmlSVUiqKNKkqpVQUaVJVSqko0qSqlFJRpElVNZqIJIvIn0Rkl4iUisg2EXmkxjxXiMgXIlImIgfs5zeKSKIOGagUoElVNc3/AicAJwLtsO7AurR6oojcBvwZeBDr6qkc4HrgZGJvPAClokqTqqqTiNwpIptFpEREvhGRi+xJ44E3jDF7jGWbMeZ5e5kOwH3AjcaYV40xJfY8y4wxVxpjfPZ8XhH5o4jsEJH9IvKEiKTa06bapeDb7FLuXhGZGRZXBxF5XkTyRWS7iNwtIi572gwRWSAiD4tIkYhsEZGT7Nd32uu7Omxd9cWRLSLv2uspFJH51dtRqi76A1H12Yx1f6oOQB4wS0S6AYuBn9un86NqnNJPArzAWw2s+3fAYGAM1qhWPYB7wqZ3tbfbA/gx8LiIZNrTHrWn9QdOAaYDM8OWnYB1b6lOwL+B/2AdCAYCVwGPiUhGBHHcBuwCOmOVtu8CvRuAaoAxRh/6iOgBLAemYV2X/z/AAqzbj+wBrrbnuQrYV2O5hVijRVUAU7BuxVIGDAibZxKw1X4+1Z7XEzb9ANatXNxYo/QPD5t2HTDPfj4D2Bg2bRRWIswJe+0gVhJtKI77sA4OA53+7PURP4+2futhVQ8RmQ78HOhrv5QBZBtjgsDjWKXHVOAa4J8i8iVWwsoWEY8xJgBgjDnJXt8urLOjzliDWX8dVsgVrIRZ7WD18rby6u0DScD2sGnbOfZur/vDnlfYMdR8LSOCOB4E7gVm29OfNMb87tuflFJH6em/qpWI9AGeAm4COhljOgKrqXHDP2NMhTHmceAQMBxYhFV6nVbP6guwEtsIY0xH+9HBGJNRzzLhy1YBfcJe6w3sjuiNNSIOY9UH32aM6Q9ciFXlcXoTtqPaEE2qqi7pWKfN+QB2Q9FI+/mtdmNSqoh47IafdsAyY0wRVv3rX0XkEhFpJyIuERljrxNjTAgrYT8sIl3sdfYQkbMbCsouJb8M3G+vuw9WaXpWY99gQ3GIyHdFZKBdZ1yMNV6rDi2o6qVJVdXKGPMN8Ceskud+rLrJBfbkcnvaPqzS3v8A3zfGbLGX/QNWorvdXnY/8HfgDqz6Veznm4DFInIYmAMMiTC8m7HqQrcAn2M1Rv2ziW+1vjgG2f+XYn0OfzXGzG3idlQboeOpKqVUFGlJVSmlokiTqlJKRZEmVaWUiiJNqkopFUWaVJVSKoo0qSqlVBRpUlVKqSjSpKqUUlGkSVUppaLo/wNJCn7q9KZENAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "subsets = (1, 1, 0.2, 1, 0.2, 0.2, 0.1)\n",
    "v = venn3(subsets=subsets, set_labels=(\"aMetagenomes\",\"mMetagenomes\",\"aSGenomes\"))\n",
    "\n",
    "dummies = ['100', '101', '110', '010', '001', '011', '111']\n",
    "\n",
    "for dummie,label in zip(dummies,labels_1):\n",
    "    v.get_label_by_id(dummie).set_text(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA on informative k-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=classification_1[(classification_1.Perc_aMetagenome>40)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "b=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome>40)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "c=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome>40)].index.values.tolist()\n",
    "informative_kmers=a+b+c\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers=all_kmers.iloc[informative_kmers]\n",
    "\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"Kmer\"])]\n",
    "\n",
    "icombined_total=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(icombined_total)\n",
    "scaled_icombined_total=scaler.transform(icombined_total)\n",
    "scaled_t_icombined_total=scaled_icombined_total.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_12 = scaled_t_icombined_total.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_12 = PCA(n_components=2)\n",
    "to_plot_12=pca_12.fit(dX_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=scaled_t_icombined_total.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.txt\", to_plot_12.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12=np.genfromtxt(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aS_dict=dict()\n",
    "aS_dict['ERR1883904']=\"Mycobacterium leprae\"\n",
    "aS_dict['ERR2204628']=\"Salmonella enterica\"\n",
    "aS_dict['ERR2862146']=\"Yersinia pestis\"\n",
    "aS_dict['SRR12548766']=\"Tannerella forsythia\"\n",
    "aS_dict['ERR4354257']=\"Yersinia pestis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species=combined_total[list(aS_dict.keys())].compute()\n",
    "species=species.iloc[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_species(row):\n",
    "    val=\"\"\n",
    "    for accession in list(aS_dict.keys()):\n",
    "        if row[accession]>0:\n",
    "            val=val+aS_dict[accession]+\", \"\n",
    "        else:\n",
    "            val=val+\"\"\n",
    "    return val\n",
    "    \n",
    "species[\"which_aSGenome\"]=species.apply(which_species,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species=species.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species.to_csv(\"Preprocessed_data/combination/species.txt\")\n",
    "species=pd.read_csv(\"Preprocessed_data/combination/species.txt\",delimiter=\",\")\n",
    "species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for each in species[\"which_aSGenome\"]: \n",
    "    if \"Salmonella enterica\" in each:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1.to_csv(\"Preprocessed_data/combination/classification_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=610\n",
    "y=5734\n",
    "z=7000\n",
    "\n",
    "informative_kmers=a+b+c\n",
    "informative_kmers.sort()\n",
    "#Make list of original indeces\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(a,x)\n",
    "aS=random.sample(b,y)\n",
    "mM=random.sample(c,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_12.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_12.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_12[0,aM_corrected], y=to_plot_12[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_12[0,aS_corrected], y=to_plot_12[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\n",
    "                        hovertext=species[\"which_aSGenome\"],\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_12[0,mM_corrected], y=to_plot_12[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Informed selection of kmers from the whole combined dataset',title_y=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of an informed subset taken from the large combined dataset. **Informative k-mers are the subset of k-mers obtained by selecting the k-mers that were present in at least 40% of the samples in one class and that were not present in any sample in the other classes**. Example: If a k-mer was present in at least 40% of the aSGenomes samples AND it was not present in any of the other samples from the other classes, this k-mer was included here. The idea behind this was to select the k-mers that were unique to each class and that could help differentiate better between them.\n",
    "\n",
    "I only plot a subsample of the total number of kmers used to build the PCA(1% of the informed subset). The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for meeting on the 14/04/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers=dict()\n",
    "icombined_total=dict()\n",
    "imatrices=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaling+ PCA of a subsample that is representative of each group in the Venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take=[]\n",
    "n=100000\n",
    "one=random.sample(intersection_1,int(0.456*n))\n",
    "two=random.sample(onlyaMetagenomes_1,int(59.275*n))\n",
    "three=random.sample(onlyaSGenomes_1,int(0.001*n))\n",
    "four=random.sample(onlymMetagenomes_1, int(16.103*n))\n",
    "five=random.sample(aMetagenomes_aSGenomes_1-intersection_1,int(4.745*n))\n",
    "six=random.sample(aMetagenomes_mMetagenomes_1-intersection_1,int(19.393*n))\n",
    "seven=random.sample(mMetagenomes_aSGenomes_1-intersection_1,int(0.026*n))\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "to_take.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers[\"RepresentativeSample\"]=all_kmers.iloc[to_take]\n",
    "\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample\"][\"Kmer\"])]\n",
    "\n",
    "imatrices[\"RepresentativeSample\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(imatrices[\"RepresentativeSample\"])\n",
    "scaled_icombined_total=scaler.transform(imatrices[\"RepresentativeSample\"])\n",
    "scaled_t_icombined_total=scaled_icombined_total.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_14 = scaled_t_icombined_total.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_14 = PCA(n_components=2)\n",
    "to_plot_14=pca_14.fit(dX_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=scaled_t_icombined_total.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=scaled_t_icombined_total.std(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/RepresentativeSample_SS_PCA.txt\", to_plot_14.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_14.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_14.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_14=np.genfromtxt(\"Preprocessed_data/combination/RepresentativeSample_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=1000\n",
    "u=1000\n",
    "v=100\n",
    "w=1000\n",
    "x=1000\n",
    "y=1000\n",
    "z=1000\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_14.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_14.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_aM_i_aS_i_mM], y=to_plot_14[1,c_aM_i_aS_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aM], y=to_plot_14[1,c_only_aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aS], y=to_plot_14[1,c_only_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_mM], y=to_plot_14[1,c_only_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aM_i_aS], y=to_plot_14[1,c_only_aM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aM_i_mM], y=to_plot_14[1,c_only_aM_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_mM_i_aS], y=to_plot_14[1,c_only_mM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Standard scaling + PCA <br>'+\\\n",
    "                  'Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 1.4% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/RepresentativeSample_SS_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization + PCA of a subsample that is representative of each group in the Venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "t_icombined_total=imatrices[\"RepresentativeSample\"].transpose()\n",
    "transformer = Binarizer().fit(t_icombined_total)  # fit does nothing.\n",
    "b_t_icombined_total=transformer.transform(t_icombined_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_15 = b_t_icombined_total\n",
    "pca_15 = PCA(n_components=2)\n",
    "to_plot_15=pca_15.fit(dX_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.txt\", to_plot_15.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15=np.genfromtxt(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=1000\n",
    "u=1000\n",
    "v=100\n",
    "w=1000\n",
    "x=1000\n",
    "y=1000\n",
    "z=1000\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_15.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_15.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_aM_i_aS_i_mM], y=to_plot_15[1,c_aM_i_aS_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aM], y=to_plot_15[1,c_only_aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aS], y=to_plot_15[1,c_only_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_mM], y=to_plot_15[1,c_only_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aM_i_aS], y=to_plot_15[1,c_only_aM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aM_i_mM], y=to_plot_15[1,c_only_aM_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_mM_i_aS], y=to_plot_15[1,c_only_mM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Binarization + PCA <br>'+\\\n",
    "                  'Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 1.4% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaling + Kernel PCA (RBF kernel) on 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take=[]\n",
    "n=1000\n",
    "one=random.sample(intersection_1,int(0.456*n))\n",
    "two=random.sample(onlyaMetagenomes_1,int(59.275*n))\n",
    "three=random.sample(onlyaSGenomes_1,int(0.001*n))\n",
    "four=random.sample(onlymMetagenomes_1, int(16.103*n))\n",
    "five=random.sample(aMetagenomes_aSGenomes_1-intersection_1,int(4.745*n))\n",
    "six=random.sample(aMetagenomes_mMetagenomes_1-intersection_1,int(19.393*n))\n",
    "seven=random.sample(mMetagenomes_aSGenomes_1-intersection_1,int(0.026*n))\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "to_take.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers[\"RepresentativeSample_small\"]=all_kmers.iloc[to_take]\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample_small\"][\"Kmer\"])]\n",
    "imatrices[\"RepresentativeSample_small\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(imatrices[\"RepresentativeSample_small\"])\n",
    "scaled_icombined_total=scaler.transform(imatrices[\"RepresentativeSample_small\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_icombined_total=scaled_icombined_total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='rbf',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed_2 = transformer.fit_transform(scaled_icombined_total)\n",
    "X_transformed_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/SS+RBF_Kernel_PCA_RepresentativeSample_small.txt\", X_transformed_2, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_2=np.genfromtxt(\"Preprocessed_data/combination/SS+RBF_Kernel_PCA_RepresentativeSample_small.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=100\n",
    "u=100\n",
    "v=1\n",
    "w=100\n",
    "x=100\n",
    "y=100\n",
    "z=26\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed_2.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed_2.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_aM_i_aS_i_mM,0], y=X_transformed_2[c_aM_i_aS_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aM,0], y=X_transformed_2[c_only_aM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aS,0], y=X_transformed_2[c_only_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_mM,0], y=X_transformed_2[c_only_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aM_i_aS,0], y=X_transformed_2[c_only_aM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aM_i_mM,0], y=X_transformed_2[c_only_aM_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_mM_i_aS,0], y=X_transformed_2[c_only_mM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Standard scaling + Kernel (RBF) PCA <br>'+\\\n",
    "                  ' Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 0.12% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/SS+RBF_Kernel_PCA_RepresentativeSample_small.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization + Kernel PCA (RBF kernel) on 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "icombined_total=imatrices[\"RepresentativeSample_small\"]\n",
    "transformer = Binarizer().fit(icombined_total)  # fit does nothing.\n",
    "b_icombined_total=transformer.transform(icombined_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='rbf',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed_3 = transformer.fit_transform(b_icombined_total)\n",
    "X_transformed_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Binarization+RBF_Kernel_PCA_RepresentativeSample_small.txt\", X_transformed_3, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_3=np.genfromtxt(\"Preprocessed_data/combination/Binarization+RBF_Kernel_PCA_RepresentativeSample_small.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=100\n",
    "u=100\n",
    "v=1\n",
    "w=100\n",
    "x=100\n",
    "y=100\n",
    "z=26\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed_3.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed_3.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_aM_i_aS_i_mM,0], y=X_transformed_3[c_aM_i_aS_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aM,0], y=X_transformed_3[c_only_aM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aS,0], y=X_transformed_3[c_only_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_mM,0], y=X_transformed_3[c_only_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aM_i_aS,0], y=X_transformed_3[c_only_aM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aM_i_mM,0], y=X_transformed_3[c_only_aM_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_mM_i_aS,0], y=X_transformed_3[c_only_mM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Binarization + Kernel (RBF) PCA <br>'+\\\n",
    "                  'Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 0.12% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/Binarization+RBF_Kernel_PCA_RepresentativeSample_small.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaling + PCA on exclusive k-mers without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=classification_1[(classification_1.Perc_aMetagenome>40)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "b=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome>40)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "c=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome>40)].index.values.tolist()\n",
    "informative_kmers=a+b+c\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "#Dataframe with labels of informative k-mers\n",
    "ikmers[\">40%_in_each_class_only\"]=all_kmers.iloc[informative_kmers]\n",
    "\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\">40%_in_each_class_only\"][\"Kmer\"])]\n",
    "\n",
    "#Matrix of informative k-mers\n",
    "imatrices[\">40%_in_each_class_only\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12=np.genfromtxt(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude=list(np.where(to_plot_16[0,:] < -0.05)[0])\n",
    "to_exclude= to_exclude + list(np.where(to_plot_16[0,:] > 0.02)[0])\n",
    "to_exclude= to_exclude + list(np.where(to_plot_16[1,:] > 0.2)[0])\n",
    "to_exclude= to_exclude + list(np.where(to_plot_16[1,:] < -0.2)[0])\n",
    "len(to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive_kmers=set(informative_kmers)-set(to_exclude)\n",
    "exclusive_kmers=list(exclusive_kmers)\n",
    "exclusive_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(to_exclude)/len(informative_kmers))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 k-mers are excluded whihc is 0.00015% of the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers[\">40%_in_each_class_only_no_outliers\"]=all_kmers.iloc[exclusive_kmers]\n",
    "ecombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\">40%_in_each_class_only_no_outliers\"][\"Kmer\"])]\n",
    "#Matrix of informative k-mers\n",
    "imatrices[\">40%_in_each_class_only_no_outliers\"]=ecombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First do standard scaling of the features\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaled_ecombined_total= StandardScaler().fit_transform(imatrices[\">40%_in_each_class_only_no_outliers\"])\n",
    "scaled_t_ecombined_total=scaled_ecombined_total.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_16 = scaled_t_ecombined_total.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_16 = PCA(n_components=2)\n",
    "to_plot_16=pca_16.fit(dX_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_SS_PCA.txt\", to_plot_16.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_16.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_16.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_16=np.genfromtxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=610\n",
    "y=5734\n",
    "z=10000\n",
    "\n",
    "#Make list of original indeces\n",
    "informative_kmers=set(informative_kmers)-set(to_exclude)\n",
    "informative_kmers=list(informative_kmers)\n",
    "informative_kmers.sort()\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(a,x)\n",
    "aS=random.sample(b,y)\n",
    "mM=random.sample(c,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_16.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_16.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_16[0,aM_corrected], y=to_plot_16[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_16[0,aS_corrected], y=to_plot_16[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_16[0,mM_corrected], y=to_plot_16[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + ' labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Exclusive kmers from the whole combined dataset without outliers',title_y=0.95)\n",
    "fig.write_html(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_SS_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization + PCA on exclusive k-mers without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "t_ecombined_total=imatrices[\">40%_in_each_class_only_no_outliers\"].transpose()\n",
    "transformer = Binarizer().fit(t_ecombined_total)  # fit does nothing.\n",
    "b_t_ecombined_total=transformer.transform(t_ecombined_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_17 = b_t_ecombined_total\n",
    "pca_17 = PCA(n_components=2)\n",
    "to_plot_17=pca_17.fit(dX_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_Binarization_PCA.txt\",\\\n",
    "           to_plot_17.components_, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_17.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_17.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_17=np.genfromtxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_Binarization_PCA.txt\",delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=610\n",
    "y=5734\n",
    "z=10000\n",
    "\n",
    "#Make list of original indeces\n",
    "informative_kmers=set(informative_kmers)-set(to_exclude)\n",
    "informative_kmers=list(informative_kmers)\n",
    "informative_kmers.sort()\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(a,x)\n",
    "aS=random.sample(b,y)\n",
    "mM=random.sample(c,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_17.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_17.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_17[0,aM_corrected], y=to_plot_17[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_17[0,aS_corrected], y=to_plot_17[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_17[0,mM_corrected], y=to_plot_17[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Binarization + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Exclusive kmers from the whole combined dataset without outliers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_Binarization_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "imatrices=dict()\n",
    "ikmers=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take=[]\n",
    "n=100000\n",
    "one=random.sample(intersection_1,int(0.456*n))\n",
    "two=random.sample(onlyaMetagenomes_1,int(59.275*n))\n",
    "three=random.sample(onlyaSGenomes_1,int(0.001*n))\n",
    "four=random.sample(onlymMetagenomes_1, int(16.103*n))\n",
    "five=random.sample(aMetagenomes_aSGenomes_1-intersection_1,int(4.745*n))\n",
    "six=random.sample(aMetagenomes_mMetagenomes_1-intersection_1,int(19.393*n))\n",
    "seven=random.sample(mMetagenomes_aSGenomes_1-intersection_1,int(0.026*n))\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "to_take.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/list_of_indeces_of_RepresentativeSample.csv\",to_take,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers[\"RepresentativeSample\"]=all_kmers.iloc[to_take]\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample\"][\"Kmer\"])]\n",
    "imatrices[\"RepresentativeSample\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imatrices[\"RepresentativeSample\"]=imatrices[\"RepresentativeSample\"].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "ecombined_total=imatrices[\"RepresentativeSample\"]\n",
    "b_ecombined_total = Binarizer().fit_transform(ecombined_total)  # fit does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/RepresentativeSample_binarized.csv\", b_ecombined_total, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ecombined_total=dd.read_table(\"Classification/RepresentativeSample_binarized.csv\",delimiter=\",\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ecombined_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1=Ancient\n",
    "#2=NonAncient\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "labels=[]\n",
    "labels=labels+[0]*len(one)\n",
    "labels=labels+[0]*len(two)\n",
    "labels=labels+[1]*len(three)\n",
    "labels=labels+[0]*len(four)\n",
    "labels=labels+[1]*len(five)\n",
    "labels=labels+[0]*len(six)\n",
    "labels=labels+[1]*len(seven)\n",
    "Data = pd.DataFrame(list(zip(to_take, labels)),\n",
    "               columns =['Index', 'Label'])\n",
    "Data.sort_values(by='Index' , inplace=True)\n",
    "Data=Data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=Data.drop(\"index\",axis=1)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/Labels_RepresentativeSample.csv\",Data[\"Label\"], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.genfromtxt(\"Classification/Labels_RepresentativeSample.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and testing with non-blockwise shuffling\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X=b_ecombined_total.compute_chunk_sizes()\n",
    "y=da.from_array(y).compute_chunk_sizes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 76.29 MiB </td> <td> 76.29 MiB </td></tr>\n",
       "    <tr><th> Shape </th><td> (9999900,) </td> <td> (9999900,) </td></tr>\n",
       "    <tr><th> Count </th><td> 2 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int64 </td><td> pandas.Series </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >9999900</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(9999900,), dtype=int64, chunksize=(9999900,), chunktype=pandas.Series>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 0.98 kiB </td> <td> 400 B </td></tr>\n",
       "    <tr><th> Shape </th><td> (125,) </td> <td> (50,) </td></tr>\n",
       "    <tr><th> Count </th><td> 10 Tasks </td><td> 3 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >125</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<add, shape=(125,), dtype=float64, chunksize=(50,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999899, 298)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 2.22 GiB </td> <td> 1.96 MiB </td></tr>\n",
       "    <tr><th> Shape </th><td> (1000623, 298) </td> <td> (860, 298) </td></tr>\n",
       "    <tr><th> Count </th><td> 9320 Tasks </td><td> 1165 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"56\" x2=\"25\" y2=\"56\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"75\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"101\" x2=\"25\" y2=\"101\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >298</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">1000623</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(1000623, 298), dtype=float64, chunksize=(860, 298), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.datasets import make_regression\n",
    "X_1, y_1 = make_regression(n_samples=125, n_features=4, chunks=50,\n",
    "                       random_state=0)\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 896 B </td> <td> 360 B </td></tr>\n",
       "    <tr><th> Shape </th><td> (112,) </td> <td> (45,) </td></tr>\n",
       "    <tr><th> Count </th><td> 25 Tasks </td><td> 3 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >112</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(112,), dtype=float64, chunksize=(45,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-18227f30ca8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/dask_ml/linear_model/glm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0msolver_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_solver_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solvers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msolver_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/dask_glm/utils.py\u001b[0m in \u001b[0;36mnormalize_inputs\u001b[0;34m(X, y, *args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'normalize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# in case they are read-only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mintercept_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m                 \u001b[0mlocal_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m             return self.sync(\n\u001b[0m\u001b[1;32m   1976\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m                 \u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             return sync(\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             )\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pasteur/sonic/scratch/public/cduitama/Software/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dask_ml.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted=lr.predict(X_test)\n",
    "y_train_predicted=lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test_predicted).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train,y_train_predicted).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 4.77 MiB </td> <td> 4.19 kiB </td></tr>\n",
       "    <tr><th> Shape </th><td> (4999598,) </td> <td> (4295,) </td></tr>\n",
       "    <tr><th> Count </th><td> 16311 Tasks </td><td> 1165 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> bool </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"6\" y1=\"0\" x2=\"6\" y2=\"25\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"18\" y1=\"0\" x2=\"18\" y2=\"25\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"31\" y1=\"0\" x2=\"31\" y2=\"25\" />\n",
       "  <line x1=\"37\" y1=\"0\" x2=\"37\" y2=\"25\" />\n",
       "  <line x1=\"44\" y1=\"0\" x2=\"44\" y2=\"25\" />\n",
       "  <line x1=\"50\" y1=\"0\" x2=\"50\" y2=\"25\" />\n",
       "  <line x1=\"56\" y1=\"0\" x2=\"56\" y2=\"25\" />\n",
       "  <line x1=\"63\" y1=\"0\" x2=\"63\" y2=\"25\" />\n",
       "  <line x1=\"69\" y1=\"0\" x2=\"69\" y2=\"25\" />\n",
       "  <line x1=\"75\" y1=\"0\" x2=\"75\" y2=\"25\" />\n",
       "  <line x1=\"82\" y1=\"0\" x2=\"82\" y2=\"25\" />\n",
       "  <line x1=\"88\" y1=\"0\" x2=\"88\" y2=\"25\" />\n",
       "  <line x1=\"94\" y1=\"0\" x2=\"94\" y2=\"25\" />\n",
       "  <line x1=\"101\" y1=\"0\" x2=\"101\" y2=\"25\" />\n",
       "  <line x1=\"107\" y1=\"0\" x2=\"107\" y2=\"25\" />\n",
       "  <line x1=\"113\" y1=\"0\" x2=\"113\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >4999598</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<gt, shape=(4999598,), dtype=bool, chunksize=(4295,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predicted=y_train_predicted.persist()\n",
    "y_train=y_train.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4295, 4999598]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d537e4c71321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Accuracy on train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4295, 4999598]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = y_train_predicted.compute()\n",
    "y_true = y_train.compute()\n",
    "#Accuracy on train set\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
