{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Metadata\" data-toc-modified-id=\"Metadata-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Metadata</a></span><ul class=\"toc-item\"><li><span><a href=\"#SRA-files-downloaded\" data-toc-modified-id=\"SRA-files-downloaded-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>SRA files downloaded</a></span></li><li><span><a href=\"#Plots-of-metadata-variables\" data-toc-modified-id=\"Plots-of-metadata-variables-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Plots of metadata variables</a></span></li><li><span><a href=\"#Homo-Sapiens-Oral-metadata\" data-toc-modified-id=\"Homo-Sapiens-Oral-metadata-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Homo Sapiens Oral metadata</a></span></li></ul></li><li><span><a href=\"#DASK\" data-toc-modified-id=\"DASK-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>DASK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Homo-sapiens-oral-(50th---5.8-GB)\" data-toc-modified-id=\"Homo-sapiens-oral-(50th---5.8-GB)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Homo sapiens oral (50th - 5.8 GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-samples\" data-toc-modified-id=\"Plot-samples-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Plot samples</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binarization-+-PCA\" data-toc-modified-id=\"Binarization-+-PCA-2.1.1.1\"><span class=\"toc-item-num\">2.1.1.1&nbsp;&nbsp;</span>Binarization + PCA</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.1.1.2\"><span class=\"toc-item-num\">2.1.1.2&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li><li><span><a href=\"#Plot-k-mers\" data-toc-modified-id=\"Plot-k-mers-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Plot k-mers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binarization-+-PCA\" data-toc-modified-id=\"Binarization-+-PCA-2.1.2.1\"><span class=\"toc-item-num\">2.1.2.1&nbsp;&nbsp;</span>Binarization + PCA</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.1.2.2\"><span class=\"toc-item-num\">2.1.2.2&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li></ul></li><li><span><a href=\"#Homo-sapiens-oral-(50th---5.8-GB)-without-human-reads\" data-toc-modified-id=\"Homo-sapiens-oral-(50th---5.8-GB)-without-human-reads-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Homo sapiens oral (50th - 5.8 GB) without human reads</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-k-mers\" data-toc-modified-id=\"Plot-k-mers-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Plot k-mers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li></ul></li><li><span><a href=\"#Homo-sapiens-oral-(25th---12GB)\" data-toc-modified-id=\"Homo-sapiens-oral-(25th---12GB)-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Homo sapiens oral (25th - 12GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-k-mers\" data-toc-modified-id=\"Plot-k-mers-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Plot k-mers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling-+-PCA\" data-toc-modified-id=\"Standard-scaling-+-PCA-2.3.1.1\"><span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Standard scaling + PCA</a></span></li></ul></li></ul></li><li><span><a href=\"#aSGenomes\" data-toc-modified-id=\"aSGenomes-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>aSGenomes</a></span></li><li><span><a href=\"#Combined-datasets\" data-toc-modified-id=\"Combined-datasets-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Combined datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Every-50th-k-mer-(8.3GB)\" data-toc-modified-id=\"Every-50th-k-mer-(8.3GB)-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Every 50th k-mer (8.3GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binarization-+-PCA-on-entire-dataset\" data-toc-modified-id=\"Binarization-+-PCA-on-entire-dataset-2.5.1.1\"><span class=\"toc-item-num\">2.5.1.1&nbsp;&nbsp;</span>Binarization + PCA on entire dataset</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA-on-informed-dataset\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-informed-dataset-2.5.1.2\"><span class=\"toc-item-num\">2.5.1.2&nbsp;&nbsp;</span>Standard scaling + PCA on informed dataset</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA-on-entire-dataset\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-entire-dataset-2.5.1.3\"><span class=\"toc-item-num\">2.5.1.3&nbsp;&nbsp;</span>Standard scaling + PCA on entire dataset</a></span></li><li><span><a href=\"#Kernel-PCA-on--arbitrary-subset\" data-toc-modified-id=\"Kernel-PCA-on--arbitrary-subset-2.5.1.4\"><span class=\"toc-item-num\">2.5.1.4&nbsp;&nbsp;</span>Kernel PCA on  arbitrary subset</a></span></li><li><span><a href=\"#Kernel-PCA-on-informed-subset\" data-toc-modified-id=\"Kernel-PCA-on-informed-subset-2.5.1.5\"><span class=\"toc-item-num\">2.5.1.5&nbsp;&nbsp;</span>Kernel PCA on informed subset</a></span></li></ul></li><li><span><a href=\"#Whole-dataset-(412GB)\" data-toc-modified-id=\"Whole-dataset-(412GB)-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Whole dataset (412GB)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling-+-PCA-on-informative-k-mers\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-informative-k-mers-2.5.2.1\"><span class=\"toc-item-num\">2.5.2.1&nbsp;&nbsp;</span>Standard scaling + PCA on informative k-mers</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Plots-for-meeting-on-the-14/04/2021\" data-toc-modified-id=\"Plots-for-meeting-on-the-14/04/2021-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Plots for meeting on the 14/04/2021</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-scaling+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram\" data-toc-modified-id=\"Standard-scaling+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Standard scaling+ PCA of a subsample that is representative of each group in the Venn diagram</a></span></li><li><span><a href=\"#Binarization-+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram\" data-toc-modified-id=\"Binarization-+-PCA-of-a-subsample-that-is-representative-of-each-group-in-the-Venn-diagram-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Binarization + PCA of a subsample that is representative of each group in the Venn diagram</a></span></li><li><span><a href=\"#Standard-scaling-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data\" data-toc-modified-id=\"Standard-scaling-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Standard scaling + Kernel PCA (RBF kernel) on 0.1% of the data</a></span></li><li><span><a href=\"#Binarization-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data\" data-toc-modified-id=\"Binarization-+-Kernel-PCA-(RBF-kernel)-on-0.1%-of-the-data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Binarization + Kernel PCA (RBF kernel) on 0.1% of the data</a></span></li><li><span><a href=\"#Standard-scaling-+-PCA-on-exclusive-k-mers-without-outliers\" data-toc-modified-id=\"Standard-scaling-+-PCA-on-exclusive-k-mers-without-outliers-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Standard scaling + PCA on exclusive k-mers without outliers</a></span></li><li><span><a href=\"#Binarization-+-PCA-on-exclusive-k-mers-without-outliers\" data-toc-modified-id=\"Binarization-+-PCA-on-exclusive-k-mers-without-outliers-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Binarization + PCA on exclusive k-mers without outliers</a></span></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Validation-set\" data-toc-modified-id=\"Validation-set-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>Validation set</a></span></li><li><span><a href=\"#SGD\" data-toc-modified-id=\"SGD-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>SGD</a></span></li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-3.7.4\"><span class=\"toc-item-num\">3.7.4&nbsp;&nbsp;</span>SVM</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aMetagenomes=pd.read_table(\"Metadata/aMetagenomes/SRR_aMetagenomes.txt\", names=[\"Accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_SRStoSRR=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes_SRStoSRR.tsv\",names=[\"archive_accession\",\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_metadata=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes.txt\")\n",
    "#Create one row per archive_accession\n",
    "aMetagenomes_metadata=aMetagenomes_metadata.assign(archive_accession=aMetagenomes_metadata['archive_accession'].str.split(',')).explode('archive_accession')\n",
    "aMetagenomes_metadata=aMetagenomes_metadata.reset_index()\n",
    "aMetagenomes_metadata=aMetagenomes_metadata[aMetagenomes_metadata[\"archive\"]==\"SRA\"]\n",
    "aMetagenomes_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter aMetagenomes_metadata with only files that were downloaded (SRA files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRA files downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aMetagenomes=pd.read_table(\"Metadata/aMetagenomes/SRR_aMetagenomes.txt\",names=[\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.merge(aMetagenomes_SRStoSRR,SRR_aMetagenomes,on=\"SRR_accession\",how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_metadata=pd.merge(aMetagenomes_metadata,aMetagenomes_SRStoSRR,on=\"archive_accession\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_metadata.to_csv(\"Metadata/aMetagenomes/aMetagenomes_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of metadata variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"sample_age\",title=\"Sample age\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"sample_host\",title=\"Sample host\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"community_type\",title=\"Community type\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aMetagenomes_metadata, x=\"material\",title=\"material\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(aMetagenomes_metadata, lat=\"latitude\", lon=\"longitude\", hover_name=\"geo_loc_name\",\n",
    "                        hover_data=[\"site_name\",\"sample_host\",\"sample_age\",\"community_type\",\"SRR_accession\"],\n",
    "                        color_discrete_sequence=[\"red\"], zoom=2, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo Sapiens Oral metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I select the SRR_accessions from the aMetagenomes_metadata DF where the sample_host is Homo sapiens and the community type is oral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_metadata=aMetagenomes_metadata[(aMetagenomes_metadata[\"sample_host\"]==\"Homo sapiens\")\\\n",
    "                                  & (aMetagenomes_metadata[\"community_type\"]==\"oral\")]\n",
    "HS_Oral_metadata=HS_Oral_metadata.reset_index()\n",
    "SRR_HS_Oral=[\"Kmer\"]+list(HS_Oral_metadata[\"SRR_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_metadata.to_csv(\"Metadata/aMetagenomes/HS_Oral_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(HS_Oral_metadata, x=\"sample_age\",title=\"Sample age\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(HS_Oral_metadata, x=\"publication_year\",title=\"Publication year\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(HS_Oral_metadata, x=\"material\",title=\"Material\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(HS_Oral_metadata, lat=\"latitude\", lon=\"longitude\", hover_name=\"geo_loc_name\",\n",
    "                        hover_data=[\"site_name\",\"sample_host\",\"sample_age\",\"community_type\",\"SRR_accession\"],\n",
    "                        color_discrete_sequence=[\"red\"], zoom=2, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "#client.shutdown()\n",
    "#client.close()\n",
    "dask.config.set({'temporary_directory': '/pasteur/sonic/scratch/public/cduitama/RascovanProject/tmp/'})\n",
    "#dask.config.set({\"optimization.fuse.ave-width\": 5})\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.get_versions(check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo sapiens oral (50th - 5.8 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size = 5.8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=dd.read_table(\"kmMatrices/aMetagenomes/HS_Oral_every50th.txt\",sep=\" \",header=None,names=SRR_HS_Oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove column of k-mers\n",
    "test=test.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test=test.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "transformer = Binarizer().fit(test)  # fit does nothing.\n",
    "b_test=transformer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_test=b_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX = b_test\n",
    "pca= PCA(n_components=2)\n",
    "to_plot=pca.fit(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Samples_bPCA.csv\", to_plot.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Samples_bPCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=to_plot[0], y=to_plot[1], color=HS_Oral_metadata[\"geo_loc_name\"], \\\n",
    "                 title=\"Binarization + PCA of samples coloured based on origin <br> Dataset: H.sapiens Oral every 50th k-mer\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we standardize the features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test)\n",
    "scaled_test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=scaled_test.mean(axis=0).compute()\n",
    "b=scaled_test.std(axis=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0:10])\n",
    "print(b[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_1 = scaled_test\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_1 = PCA(n_components=2,whiten=True)\n",
    "to_plot_1=pca_1.fit(dX_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_1.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_1.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Samples_SS_PCA.csv\", to_plot_1.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_1=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Samples_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=to_plot_1[0], y=to_plot_1[1],color=HS_Oral_metadata[\"geo_loc_name\"],\n",
    "                title=\"Standard scaling + PCA of samples coloured based on location <br> Dataset: H.sapiens Oral every 50th kmer\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot k-mers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "transformer = Binarizer().fit(t_test)  # fit does nothing.\n",
    "tb_test=transformer.transform(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "dX_5 = tb_test\n",
    "pca_5 = PCA(n_components=2)\n",
    "to_plot_5=pca_5.fit(dX_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_5.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_5.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Kmers_bPCA.csv\", to_plot_5.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_5=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Kmers_bPCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n=10000\n",
    "randomlist = random.sample(range(0, to_plot_5.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_5[0,randomlist], y=to_plot_5[1,randomlist],opacity=0.5,\\\n",
    "                 title=\"Binarization + PCA of k-mers <br> Dataset: H.sapiens Oral every 50th k-mer <br>\"+\\\n",
    "                 \" Sample: \" +str(n)+\" randomly selected k-mers \")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test)\n",
    "scaled_test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_test=scaled_test.transpose()\n",
    "scaled_t_test=scaled_t_test.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_4 = scaled_t_test\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_4 = PCA(n_components=2,whiten=True)\n",
    "to_plot_4=pca_4.fit(dX_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_4.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_4.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Kmers_SS_PCA.csv\", to_plot_4.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_plot_4=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Kmers_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "import random\n",
    "randomlist = random.sample(range(0, to_plot_4.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_4[0,randomlist], y=to_plot_4[1,randomlist],opacity=0.5,\\\n",
    "                 title=\"Standard scaling + PCA of k-mers <br> Dataset: H.sapiens Oral every 50th k-mer <br>\"+\\\n",
    "                 \" Sample: \" +str(n)+\" randomly selected k-mers \")\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo sapiens oral (50th - 5.8 GB) without human reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_aMetagenomes=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes_filesize.txt\", sep=\"\\s+\",\\\n",
    "                              names=[\"index\",\"one\",\"User\",\"Machine\",\"Size\",\"Month\",\"Day\",\"Time\",\"fastq\"])\n",
    "fs_aMetagenomes=fs_aMetagenomes.drop([\"index\",\"one\",\"User\",\"Machine\",\"Month\",\"Day\",\"Time\"],axis=1)\n",
    "fs_aMetagenomes=fs_aMetagenomes.drop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_aOralNonHuman=pd.read_table(\"Metadata/aOralNonHuman/aOralNonHuman_filesize.txt\", sep=\"\\s+\",\\\n",
    "                              names=[\"index\",\"one\",\"User\",\"Machine\",\"Size\",\"Month\",\"Day\",\"Time\",\"fastq\"])\n",
    "fs_aOralNonHuman=fs_aOralNonHuman.drop([\"index\",\"one\",\"User\",\"Machine\",\"Month\",\"Day\",\"Time\"],axis=1)\n",
    "fs_aOralNonHuman=fs_aOralNonHuman.drop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileSizes=fs_aOralNonHuman.merge(fs_aMetagenomes,how=\"left\",on=\"fastq\")\n",
    "FileSizes=FileSizes.rename(columns={\"Size_x\": \"After\", \"Size_y\": \"Before\"})\n",
    "FileSizes[\"Reduction\"]=FileSizes[\"Before\"] - FileSizes[\"After\"]\n",
    "FileSizes[\"After\"]=FileSizes[\"After\"]/1e9\n",
    "FileSizes[\"Before\"]=FileSizes[\"Before\"]/1e9\n",
    "FileSizes[\"Reduction\"]=FileSizes[\"Reduction\"]/1e9\n",
    "FileSizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "fig = px.histogram(FileSizes, x=\"Reduction\",title=\"Reduction in GB\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Before', x=FileSizes[\"fastq\"], y=FileSizes[\"Before\"]),\n",
    "    go.Bar(name='After', x=FileSizes[\"fastq\"], y=FileSizes[\"After\"])\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode=\"overlay\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a file size reduction of 29GB in total in the fastq files, but this represented only 1 GB reduction in the km-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_filtered50=dd.read_table(\"kmMatrices/aOralNonHuman/aOralNonHuman_every50th.txt\",sep=\" \",\\\n",
    "                      header=None,names=SRR_HS_Oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_filtered50=HS_Oral_filtered50.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_Oral_filtered50.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot k-mers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(HS_Oral_filtered50)\n",
    "scaled_ftest=scaler.transform(HS_Oral_filtered50)\n",
    "scaled_t_ftest=scaled_ftest.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_6 = scaled_t_ftest.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_6 = PCA(n_components=2,whiten=True)\n",
    "to_plot_6=pca_6.fit(dX_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_6.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_6.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/filteredKmers_SS_PCA.csv\", to_plot_6.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_6=np.genfromtxt(\"Preprocessed_data/combination/filteredKmers_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "n=10000\n",
    "#Generate 5 random numbers between 10 and 30\n",
    "randomlist = random.sample(range(0, to_plot_6.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_6[0,randomlist], y=to_plot_6[1,randomlist],opacity=0.3,\\\n",
    "                title=\"Standard scaling + PCA of kmers <br> Dataset: H.sapiens Oral every 50th kmer <br> Sample: Randomly selected \" \\\n",
    "                + str(n) + \"k-mers\")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homo sapiens oral (25th - 12GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=dd.read_table(\"kmMatrices/aMetagenomes/HS_Oral_every25th.txt\",sep=\" \",header=None,names=SRR_HS_Oral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove column of k-mers\n",
    "test2=test2.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot k-mers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test2)\n",
    "scaled_test2=scaler.transform(test2)\n",
    "scaled_t_test2=scaled_test2.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_7 = scaled_t_test2.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_7 = PCA(n_components=2,whiten=True)\n",
    "to_plot_7=pca_7.fit(dX_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_7.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_7.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/aMetagenomes/Every25th_SS_PCA.csv\", to_plot_7.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_7=np.genfromtxt(\"Preprocessed_data/aMetagenomes/Every25th_SS_PCA.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10000\n",
    "randomlist = random.sample(range(0, to_plot_7.shape[1]), n)\n",
    "fig = px.scatter(x=to_plot_7[0,randomlist], y=to_plot_7[1,randomlist],opacity=0.3,\\\n",
    "                title=\"Standard scaling + PCA of kmers <br> Dataset: H.sapiens Oral every 25th kmer <br> Sample: Randomly selected \" \\\n",
    "                + str(n) + \" k-mers\")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aSGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSGenomes_metadata=pd.read_table(\"Metadata/aSGenomes/aSGenomes.txt\")\n",
    "#Create one row per archive_accession\n",
    "aSGenomes_metadata=aSGenomes_metadata.assign(archive_accession=aSGenomes_metadata['archive_accession'].str.split(',')).explode('archive_accession')\n",
    "aSGenomes_metadata=aSGenomes_metadata.reset_index()\n",
    "aSGenomes_metadata=aSGenomes_metadata.drop(\"index\",axis=1)\n",
    "aSGenomes_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSGenomes_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(aMetagenomes_metadata.archive_accession))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(aSGenomes_metadata.archive_accession))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(aSGenomes_metadata.archive_accession).intersection(set(aMetagenomes_metadata.archive_accession)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 21 archive accessions that exists both in the aSGenomes dataset and the aMetagenomes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract only samples where the host is Homo sapiens and the material is calculus or tooth\n",
    "aSGenomes_HS_Oral_metadata=aSGenomes_metadata[(aSGenomes_metadata[\"sample_host\"]==\"Homo sapiens\")\n",
    "                                              &(aSGenomes_metadata[\"material\"]==(\"dental calculus\"or \"tooth\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aSGenomes_HS_Oral_metadata, x=\"singlegenome_species\",\\\n",
    "                   title=\"aSingle genome species\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined dataset refers to the K-mer matrix of abundanced built from the following dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition of the combined dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | Number of SRR accessions | Percentage of total number | Fastq total File size (GB) | Percentage of total size |\n",
    "| :---: | :---: | :---: | :---: | :---: | \n",
    "| aMetagenomes | 281 | 94.3% | 338.03 | 86.6% |\n",
    "| aSGenomes | 5 | 1.7% | 9.3 | 2.4% |\n",
    "| mMetagenomes | 12 | 4.0% | 43 | 11.0% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 50th k-mer (8.3GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification=pd.read_csv(\"Preprocessed_data/combination/classification.csv\")\n",
    "classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_HS_Oral_filesizes=pd.read_table(\"Metadata/aMetagenomes/aMetagenomes_HS_Oral_filesizes.txt\",header=None,names=[\"Size\",\"File\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toG(row):\n",
    "    \n",
    "    if \"K\" in row[\"Size\"]:\n",
    "        val = row[\"Size\"].replace(\"K\",\"\")\n",
    "        val = float(val)\n",
    "        val = val/1000000\n",
    "        \n",
    "    elif \"M\" in row[\"Size\"]:\n",
    "        val = row[\"Size\"].replace(\"M\",\"\")\n",
    "        val = float(val)\n",
    "        val = val/1000\n",
    "        \n",
    "    else:\n",
    "        val = row[\"Size\"].replace(\"G\",\"\")\n",
    "        val = float(val)\n",
    "        \n",
    "    return val\n",
    "\n",
    "aMetagenomes_HS_Oral_filesizes[\"Size_GB\"]=aMetagenomes_HS_Oral_filesizes.apply(toG,axis=1)\n",
    "aMetagenomes_HS_Oral_filesizes=aMetagenomes_HS_Oral_filesizes.drop(\"Size\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aMetagenomes_HS_Oral_filesizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aSGenomes_HS_Oral=pd.read_csv(\"Metadata/aSGenomes/aSGenomes_HS_Oral_SRR.txt\",header=None,names=[\"SRR_accession\"])\n",
    "SRR_mMetagenomes_HS_Oral=pd.read_csv(\"Metadata/mMetagenomes/Oral_HS_mMetagenomes_SRR.txt\",\\\n",
    "                                     header=None,names=[\"SRR_accession\"])\n",
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "SRR_HS_Oral_aSGenomes=list(SRR_aSGenomes_HS_Oral.SRR_accession)\n",
    "SRR_HS_Oral_mMetagenomes=list(SRR_mMetagenomes_HS_Oral.SRR_accession)\n",
    "names=SRR_HS_Oral+SRR_HS_Oral_aSGenomes+SRR_HS_Oral_mMetagenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aMetagenomes=len(SRR_HS_Oral_aMetagenomes)\n",
    "n_aSGenomes=len(SRR_HS_Oral_aSGenomes)\n",
    "n_mMetagenomes=len(SRR_HS_Oral_mMetagenomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=combined.drop(\"Kmer\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of times the kmer K appears in the samples of \n",
    "aMetagenomes, aSGenomes and mMetagenomes respectively?\n",
    "The kmer k is in n% of the samples of xGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "total_aMetagenomes=(combined[SRR_HS_Oral_aMetagenomes] != 0).astype(int).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aSGenomes=(combined[SRR_HS_Oral_aSGenomes] != 0).astype(int).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mMetagenomes=(combined[SRR_HS_Oral_mMetagenomes] != 0).astype(int).sum(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_aMetagenomes=(total_aMetagenomes/n_aMetagenomes)*100\n",
    "color_aSGenomes=(total_aSGenomes/n_aSGenomes)*100\n",
    "color_mMetagenomes=(total_mMetagenomes/n_mMetagenomes)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification=pd.DataFrame()\n",
    "classification[\"Perc_aMetagenome\"]=color_aMetagenomes\n",
    "classification[\"Perc_aSGenome\"]=color_aSGenomes\n",
    "classification[\"Perc_mMetagenome\"]=color_mMetagenomes\n",
    "classification=classification.reset_index()\n",
    "classification=classification.drop(\"index\",axis=1)\n",
    "classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "classification.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.to_csv(\"Preprocessed_data/combination/classification.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification=classification.reset_index()\n",
    "classification=classification.drop(\"index\",axis=1)\n",
    "mMetagenomes=set(classification[(classification.Perc_mMetagenome > float(0))].index.values.tolist())\n",
    "aMetagenomes=set(classification[(classification.Perc_aMetagenome > float(0))].index.values.tolist())\n",
    "aSGenomes=set(classification[(classification.Perc_aSGenome > float(0))].index.values.tolist())\n",
    "intersection=set.intersection(aMetagenomes,mMetagenomes,aSGenomes)\n",
    "aMetagenomes_aSGenomes=aMetagenomes.intersection(aSGenomes)\n",
    "aMetagenomes_mMetagenomes=aMetagenomes.intersection(mMetagenomes)\n",
    "mMetagenomes_aSGenomes=mMetagenomes.intersection(aSGenomes)\n",
    "onlymMetagenomes=mMetagenomes-mMetagenomes_aSGenomes-aMetagenomes_mMetagenomes\n",
    "onlyaSGenomes=aSGenomes-mMetagenomes_aSGenomes-aMetagenomes_aSGenomes\n",
    "onlyaMetagenomes=aMetagenomes-aMetagenomes_aSGenomes-aMetagenomes_mMetagenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(mMetagenomes_aSGenomes)+len(aMetagenomes_aSGenomes)+len(aMetagenomes_mMetagenomes)-\\\n",
    "2*len(intersection)+len(onlyaSGenomes)+len(onlymMetagenomes)+len(onlyaMetagenomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "labels.append(str(round((len(onlyaMetagenomes)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(aMetagenomes_aSGenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(aMetagenomes_mMetagenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(onlymMetagenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(onlyaSGenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(mMetagenomes_aSGenomes-intersection)/classification.shape[0])*100,3))+\"%\")\n",
    "labels.append(str(round((len(intersection)/classification.shape[0])*100,3))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition of the combined dataset of every 50th k-mers based on assigned labels as aMetagenomes, mMetagenomes and aSGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "subsets = (1, 1, 0.2, 1, 0.2, 0.2, 0.1)\n",
    "v = venn3(subsets=subsets, set_labels=(\"aMetagenomes\",\"mMetagenomes\",\"aSGenomes\"))\n",
    "\n",
    "dummies = ['100', '101', '110', '010', '001', '011', '111']\n",
    "\n",
    "for dummie,label in zip(dummies,labels):\n",
    "    v.get_label_by_id(dummie).set_text(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_kmers=list(set.union(onlyaMetagenomes,onlyaSGenomes,onlymMetagenomes))\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(informative_kmers)/classification.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization + PCA on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "combined=combined.drop(\"Kmer\",axis=1).values\n",
    "combined_t=combined.transpose()\n",
    "transformer = Binarizer().fit(combined_t)  # fit does nothing.\n",
    "t_b_combined=transformer.transform(combined_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_10 = t_b_combined\n",
    "pca_10 = PCA(n_components=2)\n",
    "to_plot_10=pca_10.fit(dX_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_10.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_10.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_Binarized_PCA.txt\", to_plot_10.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_10=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_Binarized_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onlyaMetagenomes))\n",
    "print(len(onlyaSGenomes))\n",
    "print(len(onlymMetagenomes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "x=80000\n",
    "y=105\n",
    "z=2200\n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_10[0,aM], y=to_plot_10[1,aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_10[0,aS], y=to_plot_10[1,aS],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_10[0,mM], y=to_plot_10[1,mM],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Binarization + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Combined dataset every 50th kmer',title_y=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "\n",
    "This plot is the result of the binarization and posterior PCA of the combined dataset with every 50th k-mer.\n",
    "I only plot a subsample of the total number of kmers(0.08% of the total)\n",
    "The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA on informed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Preprocessed_data/combination/informative_kmers.txt', 'w') as f:\n",
    "    for item in [x+1 for x in informative_kmers]:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_kmers=list(set.union(onlyaMetagenomes,onlyaSGenomes,onlymMetagenomes))\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers=all_kmers.iloc[informative_kmers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icombined=combined[combined[\"Kmer\"].isin(ikmers[\"Kmer\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icombined=icombined.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(icombined)\n",
    "scaled_icombined=scaler.transform(icombined)\n",
    "scaled_t_icombined=scaled_icombined.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_icombined.mean(axis=1).compute()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_11 = scaled_t_icombined.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_11 = PCA(n_components=2)\n",
    "to_plot_11=pca_11.fit(dX_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_11.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_11.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_icombined_SS_PCA.txt\", to_plot_11.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_plot_11=np.genfromtxt(\"Preprocessed_data/combination/Kmers_icombined_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onlyaMetagenomes))\n",
    "print(len(onlyaSGenomes))\n",
    "print(len(onlymMetagenomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=80000\n",
    "y=105\n",
    "z=22000\n",
    "\n",
    "#Make list of original indeces\n",
    "informative_kmers=list(set.union(onlyaMetagenomes,onlyaSGenomes,onlymMetagenomes))\n",
    "informative_kmers.sort()\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_11.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_11.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_11[0,aM_corrected], y=to_plot_11[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_11[0,aS_corrected], y=to_plot_11[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_11[0,mM_corrected], y=to_plot_11[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Informed selection of kmers from combined dataset every 50th',title_y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of an informed subset taken from the combined dataset with every 50th k-mer. This informed subset was obtained by selecting the k-mers that were present only in one of the classes and not the other. Example: If a k-mer was present in at least one of the aSGenomes AND it was not present in any of the other samples from the other classes, this k-mer was included here. The idea behind this was to **select the k-mers that were unique to each class and that could help differentiate better between them**. \n",
    "\n",
    "I only plot a subsample of the total number of kmers(0.1% of the informed subset)\n",
    "The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "combined=combined.drop(\"Kmer\",axis=1).values\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(combined)\n",
    "scaled_combined=scaler.transform(combined)\n",
    "scaled_t_combined=scaled_combined.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_t_combined.mean(axis=1).compute()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_9 = scaled_t_combined.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_9 = PCA(n_components=2)\n",
    "to_plot_9=pca_9.fit(dX_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_SS_PCA.txt\", to_plot_9.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_plot_9.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_9.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_9=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onlyaMetagenomestagenomes))\n",
    "ponlyaSGenomes(onlyaSGenomes))\n",
    "print(len(onlymMetagenomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "x=8000\n",
    "y=100\n",
    "z=2200\n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aM], y=to_plot_9[1,aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aS], y=to_plot_9[1,aS],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,mM], y=to_plot_9[1,mM],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Combined dataset every 50th kmer',title_y=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of the entire combined dataset with every 50th k-mer.\n",
    "\n",
    "I only plot a subsample of the total number of kmers(0.023% of the combined dataset). The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "x=8000\n",
    "y=100\n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aM], y=to_plot_9[1,aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_9[0,aS], y=to_plot_9[1,aS],mode=\"markers\",\\\n",
    "                         legendgroup=\"aSGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  '<br>Dataset: Combined dataset every 50th kmer',title_y=0.95)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of the entire combined dataset with every 50th k-mer.\n",
    "\n",
    "I only plot a subsample of the total number of kmers(0.1% of the combined dataset) and they are either labelled as aMetagenomes or aSGenomes. The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome. **Here the idea is to see where do the k-mers belonging to aMetagenomes fall with respect to the k-mers belonging to aSGenomes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA on  arbitrary subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined=scaled_combined.compute_chunk_sizes()\n",
    "scaled_combined_subset=scaled_combined[0::100].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset=scaled_combined_subset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset.mean(axis=0)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset.std(axis=0)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='linear',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed = transformer.fit_transform(scaled_combined_subset)\n",
    "    X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This subset is 1% of the original combined dataset where we take 1 out of every 50 kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA.txt\", X_transformed, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces=classification.iloc[0::100].index.values.tolist()\n",
    "print(len(set(indeces).intersection(set(onlyaMetagenomes))))\n",
    "print(len(set(indeces).intersection(set(onlyaSGenomes))))\n",
    "print(len(set(indeces).intersection(set(onlymMetagenomes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=8000\n",
    "y=0\n",
    "z=2000\n",
    "\n",
    "#Make list of original indeces\n",
    "indeces=classification.iloc[0::100].index.values.tolist()\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_transformed[aM_corrected,0], y=X_transformed[aM_corrected,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed[aS_corrected,0], y=X_transformed[aS_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed[mM_corrected,0], y=X_transformed[mM_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Standard scaling + KernelPCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: 1% of the original combined dataset',title_y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior Kernel PCA of a randomly selected subset of k-mers taken from the  combined dataset with every 50th k-mer. This subset represents 1% of k-mers on the original dataset.\n",
    "\n",
    "I only plot a subsample of the total number of kmers(7.1% % of the randomly selected subset of k-mers) and they are either labelled as aMetagenomes or mMetagenomes. The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome. **Here the idea is to see wether a non-linear dimensionality reduction method could better separate the classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA on informed subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I take only the kmers that are unique to each dataset (aSGenome, aMetagenome and mMetagenome) but also that are present in >40% of the samples from each dataset respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=classification[(classification.Perc_aMetagenome>40)&\\\n",
    "               (classification.Perc_aSGenome==0)&(classification.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "b=classification[(classification.Perc_aMetagenome==0)&\\\n",
    "               (classification.Perc_aSGenome>40)&(classification.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "c=classification[(classification.Perc_aMetagenome==0)&\\\n",
    "               (classification.Perc_aSGenome==0)&(classification.Perc_mMetagenome>40)].index.values.tolist()\n",
    "informative_kmers=a+b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=dd.read_table(\"kmMatrices/combination/combination_every50th.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers=all_kmers.iloc[informative_kmers]\n",
    "\n",
    "icombined=combined[combined[\"Kmer\"].isin(ikmers[\"Kmer\"])]\n",
    "\n",
    "icombined=icombined.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset_1=icombined.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_combined_subset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='linear',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed_1 = transformer.fit_transform(scaled_combined_subset_1)\n",
    "    X_transformed_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA_1.txt\", X_transformed_1, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_1=np.genfromtxt(\"Preprocessed_data/combination/Kmers_combined_SS_KernelPCA_1.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=14\n",
    "y=100\n",
    "z=1400\n",
    "\n",
    "informative_kmers=a+b+c\n",
    "#Make list of original indeces\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(onlyaMetagenomes,x)\n",
    "aS=random.sample(onlyaSGenomes,y)\n",
    "mM=random.sample(onlymMetagenomes,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed_1.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed_1.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=X_transformed_1[aM_corrected,0], y=X_transformed_1[aM_corrected,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=1,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed_1[aS_corrected,0], y=X_transformed_1[aS_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=X_transformed_1[mM_corrected,0], y=X_transformed_1[mM_corrected,1],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + KernelPCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Informed selection of kmers from combined dataset every 50th',title_y=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is this plot?***\n",
    "\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of an informed subset taken from the combined dataset with every 50th k-mer. This informed subset was obtained this time by ***selecting the k-mers that were present in at least 40% of the samples in one class and that were not present in any sample in the other classes***. Example: If a k-mer was present in at least 40% of the aSGenomes samples AND it was not present in any of the other samples from the other classes, this k-mer was included here. The idea behind this was to select the k-mers that were unique to each class and that could help differentiate better between them.\n",
    "\n",
    "\n",
    "I only plot a subsample of the total number of kmers(1% of the informed subset) The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole dataset (412GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in this part of the code is to see if the composition of the original dataset varies a lot with respect to the composition of the combined dataset with every 50th k-mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aSGenomes_HS_Oral=pd.read_csv(\"Metadata/aSGenomes/aSGenomes_HS_Oral_SRR.txt\",header=None,names=[\"SRR_accession\"])\n",
    "SRR_mMetagenomes_HS_Oral=pd.read_csv(\"Metadata/mMetagenomes/Oral_HS_mMetagenomes_SRR.txt\",\\\n",
    "                                     header=None,names=[\"SRR_accession\"])\n",
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "SRR_HS_Oral_aSGenomes=list(SRR_aSGenomes_HS_Oral.SRR_accession)\n",
    "SRR_HS_Oral_mMetagenomes=list(SRR_mMetagenomes_HS_Oral.SRR_accession)\n",
    "names=SRR_HS_Oral+SRR_HS_Oral_aSGenomes+SRR_HS_Oral_mMetagenomes\n",
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "total_aMetagenomes_1=(combined_total[SRR_HS_Oral_aMetagenomes] != 0).astype(int).sum(axis=1).compute()\n",
    "\n",
    "total_aSGenomes_1=(combined_total[SRR_HS_Oral_aSGenomes] != 0).astype(int).sum(axis=1).compute()\n",
    "\n",
    "total_mMetagenomes_1=(combined_total[SRR_HS_Oral_mMetagenomes] != 0).astype(int).sum(axis=1).compute()\n",
    "\n",
    "color_aMetagenomes_1=(total_aMetagenomes_1/n_aMetagenomes)*100\n",
    "color_aSGenomes_1=(total_aSGenomes_1/n_aSGenomes)*100\n",
    "color_mMetagenomes_1=(total_mMetagenomes_1/n_mMetagenomes)*100\n",
    "\n",
    "classification_1=pd.DataFrame()\n",
    "classification_1[\"Perc_aMetagenome\"]=color_aMetagenomes_1\n",
    "classification_1[\"Perc_aSGenome\"]=color_aSGenomes_1\n",
    "classification_1[\"Perc_mMetagenome\"]=color_mMetagenomes_1\n",
    "\n",
    "classification_1=classification_1.reset_index()\n",
    "classification_1=classification_1.drop(\"index\",axis=1)\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "classification_1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1.to_csv(\"Preprocessed_data/combination/classification_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1=pd.read_csv(\"Preprocessed_data/combination/classification_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mMetagenomes_1=set(classification_1[(classification_1.Perc_mMetagenome > float(0))].index.values.tolist())\n",
    "aMetagenomes_1=set(classification_1[(classification_1.Perc_aMetagenome > float(0))].index.values.tolist())\n",
    "aSGenomes_1=set(classification_1[(classification_1.Perc_aSGenome > float(0))].index.values.tolist())\n",
    "intersection_1=set.intersection(aMetagenomes_1,mMetagenomes_1,aSGenomes_1)\n",
    "aMetagenomes_aSGenomes_1=aMetagenomes_1.intersection(aSGenomes_1)\n",
    "aMetagenomes_mMetagenomes_1=aMetagenomes_1.intersection(mMetagenomes_1)\n",
    "mMetagenomes_aSGenomes_1=mMetagenomes_1.intersection(aSGenomes_1)\n",
    "onlymMetagenomes_1=mMetagenomes_1-mMetagenomes_aSGenomes_1-aMetagenomes_mMetagenomes_1\n",
    "onlyaSGenomes_1=aSGenomes_1-mMetagenomes_aSGenomes_1-aMetagenomes_aSGenomes_1\n",
    "onlyaMetagenomes_1=aMetagenomes_1-aMetagenomes_aSGenomes_1-aMetagenomes_mMetagenomes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mMetagenomes_aSGenomes_1)+len(aMetagenomes_aSGenomes_1)+len(aMetagenomes_mMetagenomes_1)-\\\n",
    "2*len(intersection_1)+len(onlyaSGenomes_1)+len(onlymMetagenomes_1)+len(onlyaMetagenomes_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1=[]\n",
    "labels_1.append(str(round((len(onlyaMetagenomes_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(aMetagenomes_aSGenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(aMetagenomes_mMetagenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(onlymMetagenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(onlyaSGenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(mMetagenomes_aSGenomes_1-intersection_1)/classification_1.shape[0])*100,3))+\"%\")\n",
    "labels_1.append(str(round((len(intersection_1)/classification_1.shape[0])*100,3))+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Composition of the combined dataset of every 50th k-mers based on assigned labels as aMetagenomes, mMetagenomes and aSGenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "subsets = (1, 1, 0.2, 1, 0.2, 0.2, 0.1)\n",
    "v = venn3(subsets=subsets, set_labels=(\"aMetagenomes\",\"mMetagenomes\",\"aSGenomes\"))\n",
    "\n",
    "dummies = ['100', '101', '110', '010', '001', '011', '111']\n",
    "\n",
    "for dummie,label in zip(dummies,labels_1):\n",
    "    v.get_label_by_id(dummie).set_text(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaling + PCA on informative k-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=classification_1[(classification_1.Perc_aMetagenome>40)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "b=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome>40)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "c=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome>40)].index.values.tolist()\n",
    "informative_kmers=a+b+c\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers=all_kmers.iloc[informative_kmers]\n",
    "\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"Kmer\"])]\n",
    "\n",
    "icombined_total=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(icombined_total)\n",
    "scaled_icombined_total=scaler.transform(icombined_total)\n",
    "scaled_t_icombined_total=scaled_icombined_total.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_12 = scaled_t_icombined_total.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_12 = PCA(n_components=2)\n",
    "to_plot_12=pca_12.fit(dX_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=scaled_t_icombined_total.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.txt\", to_plot_12.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12=np.genfromtxt(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aS_dict=dict()\n",
    "aS_dict['ERR1883904']=\"Mycobacterium leprae\"\n",
    "aS_dict['ERR2204628']=\"Salmonella enterica\"\n",
    "aS_dict['ERR2862146']=\"Yersinia pestis\"\n",
    "aS_dict['SRR12548766']=\"Tannerella forsythia\"\n",
    "aS_dict['ERR4354257']=\"Yersinia pestis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species=combined_total[list(aS_dict.keys())].compute()\n",
    "species=species.iloc[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_species(row):\n",
    "    val=\"\"\n",
    "    for accession in list(aS_dict.keys()):\n",
    "        if row[accession]>0:\n",
    "            val=val+aS_dict[accession]+\", \"\n",
    "        else:\n",
    "            val=val+\"\"\n",
    "    return val\n",
    "    \n",
    "species[\"which_aSGenome\"]=species.apply(which_species,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species=species.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species.to_csv(\"Preprocessed_data/combination/species.txt\")\n",
    "species=pd.read_csv(\"Preprocessed_data/combination/species.txt\",delimiter=\",\")\n",
    "species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for each in species[\"which_aSGenome\"]: \n",
    "    if \"Salmonella enterica\" in each:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1.to_csv(\"Preprocessed_data/combination/classification_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=610\n",
    "y=5734\n",
    "z=7000\n",
    "\n",
    "informative_kmers=a+b+c\n",
    "informative_kmers.sort()\n",
    "#Make list of original indeces\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(a,x)\n",
    "aS=random.sample(b,y)\n",
    "mM=random.sample(c,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_12.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_12.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_12[0,aM_corrected], y=to_plot_12[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_12[0,aS_corrected], y=to_plot_12[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\n",
    "                        hovertext=species[\"which_aSGenome\"],\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_12[0,mM_corrected], y=to_plot_12[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.3,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Informed selection of kmers from the whole combined dataset',title_y=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this plot?**\n",
    "\n",
    "This plot is the result of the Standard scaling (normalizing the samples) and posterior PCA of an informed subset taken from the large combined dataset. **Informative k-mers are the subset of k-mers obtained by selecting the k-mers that were present in at least 40% of the samples in one class and that were not present in any sample in the other classes**. Example: If a k-mer was present in at least 40% of the aSGenomes samples AND it was not present in any of the other samples from the other classes, this k-mer was included here. The idea behind this was to select the k-mers that were unique to each class and that could help differentiate better between them.\n",
    "\n",
    "I only plot a subsample of the total number of kmers used to build the PCA(1% of the informed subset). The labels are assigned according to our own classification of the k-mers, this is, if they belonged to the samples we consider as aMetagenomes then their label is aMetagenome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for meeting on the 14/04/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers=dict()\n",
    "icombined_total=dict()\n",
    "imatrices=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaling+ PCA of a subsample that is representative of each group in the Venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take=[]\n",
    "n=100000\n",
    "one=random.sample(intersection_1,int(0.456*n))\n",
    "two=random.sample(onlyaMetagenomes_1,int(59.275*n))\n",
    "three=random.sample(onlyaSGenomes_1,int(0.001*n))\n",
    "four=random.sample(onlymMetagenomes_1, int(16.103*n))\n",
    "five=random.sample(aMetagenomes_aSGenomes_1-intersection_1,int(4.745*n))\n",
    "six=random.sample(aMetagenomes_mMetagenomes_1-intersection_1,int(19.393*n))\n",
    "seven=random.sample(mMetagenomes_aSGenomes_1-intersection_1,int(0.026*n))\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "to_take.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers[\"RepresentativeSample\"]=all_kmers.iloc[to_take]\n",
    "\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample\"][\"Kmer\"])]\n",
    "\n",
    "imatrices[\"RepresentativeSample\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(imatrices[\"RepresentativeSample\"])\n",
    "scaled_icombined_total=scaler.transform(imatrices[\"RepresentativeSample\"])\n",
    "scaled_t_icombined_total=scaled_icombined_total.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_14 = scaled_t_icombined_total.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_14 = PCA(n_components=2)\n",
    "to_plot_14=pca_14.fit(dX_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=scaled_t_icombined_total.mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=scaled_t_icombined_total.std(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/RepresentativeSample_SS_PCA.txt\", to_plot_14.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_14.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_14.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_14=np.genfromtxt(\"Preprocessed_data/combination/RepresentativeSample_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=1000\n",
    "u=1000\n",
    "v=100\n",
    "w=1000\n",
    "x=1000\n",
    "y=1000\n",
    "z=1000\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_14.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_14.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_aM_i_aS_i_mM], y=to_plot_14[1,c_aM_i_aS_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aM], y=to_plot_14[1,c_only_aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aS], y=to_plot_14[1,c_only_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_mM], y=to_plot_14[1,c_only_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aM_i_aS], y=to_plot_14[1,c_only_aM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_aM_i_mM], y=to_plot_14[1,c_only_aM_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_14[0,c_only_mM_i_aS], y=to_plot_14[1,c_only_mM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.7,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Standard scaling + PCA <br>'+\\\n",
    "                  'Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 1.4% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/RepresentativeSample_SS_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization + PCA of a subsample that is representative of each group in the Venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "t_icombined_total=imatrices[\"RepresentativeSample\"].transpose()\n",
    "transformer = Binarizer().fit(t_icombined_total)  # fit does nothing.\n",
    "b_t_icombined_total=transformer.transform(t_icombined_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_15 = b_t_icombined_total\n",
    "pca_15 = PCA(n_components=2)\n",
    "to_plot_15=pca_15.fit(dX_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.txt\", to_plot_15.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15=np.genfromtxt(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=1000\n",
    "u=1000\n",
    "v=100\n",
    "w=1000\n",
    "x=1000\n",
    "y=1000\n",
    "z=1000\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_15.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_15.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_aM_i_aS_i_mM], y=to_plot_15[1,c_aM_i_aS_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aM], y=to_plot_15[1,c_only_aM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aS], y=to_plot_15[1,c_only_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_mM], y=to_plot_15[1,c_only_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aM_i_aS], y=to_plot_15[1,c_only_aM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_aM_i_mM], y=to_plot_15[1,c_only_aM_i_mM],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=to_plot_15[0,c_only_mM_i_aS], y=to_plot_15[1,c_only_mM_i_aS],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Binarization + PCA <br>'+\\\n",
    "                  'Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 1.4% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaling + Kernel PCA (RBF kernel) on 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take=[]\n",
    "n=1000\n",
    "one=random.sample(intersection_1,int(0.456*n))\n",
    "two=random.sample(onlyaMetagenomes_1,int(59.275*n))\n",
    "three=random.sample(onlyaSGenomes_1,int(0.001*n))\n",
    "four=random.sample(onlymMetagenomes_1, int(16.103*n))\n",
    "five=random.sample(aMetagenomes_aSGenomes_1-intersection_1,int(4.745*n))\n",
    "six=random.sample(aMetagenomes_mMetagenomes_1-intersection_1,int(19.393*n))\n",
    "seven=random.sample(mMetagenomes_aSGenomes_1-intersection_1,int(0.026*n))\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "to_take.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers[\"RepresentativeSample_small\"]=all_kmers.iloc[to_take]\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample_small\"][\"Kmer\"])]\n",
    "imatrices[\"RepresentativeSample_small\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(imatrices[\"RepresentativeSample_small\"])\n",
    "scaled_icombined_total=scaler.transform(imatrices[\"RepresentativeSample_small\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_icombined_total=scaled_icombined_total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='rbf',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed_2 = transformer.fit_transform(scaled_icombined_total)\n",
    "X_transformed_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/SS+RBF_Kernel_PCA_RepresentativeSample_small.txt\", X_transformed_2, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_2=np.genfromtxt(\"Preprocessed_data/combination/SS+RBF_Kernel_PCA_RepresentativeSample_small.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=100\n",
    "u=100\n",
    "v=1\n",
    "w=100\n",
    "x=100\n",
    "y=100\n",
    "z=26\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed_2.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed_2.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_aM_i_aS_i_mM,0], y=X_transformed_2[c_aM_i_aS_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aM,0], y=X_transformed_2[c_only_aM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aS,0], y=X_transformed_2[c_only_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_mM,0], y=X_transformed_2[c_only_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aM_i_aS,0], y=X_transformed_2[c_only_aM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_aM_i_mM,0], y=X_transformed_2[c_only_aM_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_2[c_only_mM_i_aS,0], y=X_transformed_2[c_only_mM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Standard scaling + Kernel (RBF) PCA <br>'+\\\n",
    "                  ' Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 0.12% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/SS+RBF_Kernel_PCA_RepresentativeSample_small.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization + Kernel PCA (RBF kernel) on 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "icombined_total=imatrices[\"RepresentativeSample_small\"]\n",
    "transformer = Binarizer().fit(icombined_total)  # fit does nothing.\n",
    "b_icombined_total=transformer.transform(icombined_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.decomposition import KernelPCA\n",
    "transformer = KernelPCA(n_components=2, kernel='rbf',n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    X_transformed_3 = transformer.fit_transform(b_icombined_total)\n",
    "X_transformed_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Binarization+RBF_Kernel_PCA_RepresentativeSample_small.txt\", X_transformed_3, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed_3=np.genfromtxt(\"Preprocessed_data/combination/Binarization+RBF_Kernel_PCA_RepresentativeSample_small.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "t=100\n",
    "u=100\n",
    "v=1\n",
    "w=100\n",
    "x=100\n",
    "y=100\n",
    "z=26\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM_i_aS_i_mM=random.sample(one,t)\n",
    "only_aM=random.sample(two,u)\n",
    "only_aS=random.sample(three,v)\n",
    "only_mM=random.sample(four,w)\n",
    "only_aM_i_aS=random.sample(five, x)\n",
    "only_aM_i_mM=random.sample(six,y)\n",
    "only_mM_i_aS=random.sample(seven,z)\n",
    "\n",
    "#There are the old indeces that were subsampled\n",
    "indeces=to_take\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,X_transformed_3.shape[0])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,X_transformed_3.shape[0])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "#Corrected indeces\n",
    "c_aM_i_aS_i_mM=[new2old[x] for x in aM_i_aS_i_mM]\n",
    "c_only_aM=[new2old[x] for x in only_aM ]\n",
    "c_only_aS=[new2old[x] for x in only_aS]\n",
    "c_only_mM=[new2old[x] for x in only_mM]\n",
    "c_only_aM_i_aS=[new2old[x] for x in only_aM_i_aS]\n",
    "c_only_aM_i_mM=[new2old[x] for x in only_aM_i_mM]\n",
    "c_only_mM_i_aS=[new2old[x] for x in only_mM_i_aS]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_aM_i_aS_i_mM,0], y=X_transformed_3[c_aM_i_aS_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"Intersection\",name=\"Intersection\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aM,0], y=X_transformed_3[c_only_aM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome exclusively\",name=\"aMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aS,0], y=X_transformed_3[c_only_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aSGenome exclusively\",name=\"aSGenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_mM,0], y=X_transformed_3[c_only_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome exclusively\",name=\"mMetagenome exclusively\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aM_i_aS,0], y=X_transformed_3[c_only_aM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and aSGenome\",name=\"aMetagenome and aSGenome\",\\\n",
    "                 marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_aM_i_mM,0], y=X_transformed_3[c_only_aM_i_mM,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome and mMetagenome\",\\\n",
    "                         name=\"aMetagenome and mMetagenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=X_transformed_3[c_only_mM_i_aS,0], y=X_transformed_3[c_only_mM_i_aS,1],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"mMetagenome and aSGenome\",\\\n",
    "                         name=\"mMetagenome and aSGenome\",marker=dict(opacity=0.9,size=5)))\n",
    "\n",
    "#fig.update_traces(orientation=\"v\")\n",
    "\n",
    "fig.update_layout(title='Binarization + Kernel (RBF) PCA <br>'+\\\n",
    "                  'Sample to plot: Representative selection of every group in the Venn Diagram'+\\\n",
    "                  '<br>Dataset: 0.12% of the 700M K-mers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/Binarization+RBF_Kernel_PCA_RepresentativeSample_small.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard scaling + PCA on exclusive k-mers without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=classification_1[(classification_1.Perc_aMetagenome>40)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "b=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome>40)&(classification_1.Perc_mMetagenome==0)].index.values.tolist()\n",
    "\n",
    "c=classification_1[(classification_1.Perc_aMetagenome==0)&\\\n",
    "               (classification_1.Perc_aSGenome==0)&(classification_1.Perc_mMetagenome>40)].index.values.tolist()\n",
    "informative_kmers=a+b+c\n",
    "informative_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "#Dataframe with labels of informative k-mers\n",
    "ikmers[\">40%_in_each_class_only\"]=all_kmers.iloc[informative_kmers]\n",
    "\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\">40%_in_each_class_only\"][\"Kmer\"])]\n",
    "\n",
    "#Matrix of informative k-mers\n",
    "imatrices[\">40%_in_each_class_only\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_12=np.genfromtxt(\"Preprocessed_data/combination/Kmers_icombined_total_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude=list(np.where(to_plot_16[0,:] < -0.05)[0])\n",
    "to_exclude= to_exclude + list(np.where(to_plot_16[0,:] > 0.02)[0])\n",
    "to_exclude= to_exclude + list(np.where(to_plot_16[1,:] > 0.2)[0])\n",
    "to_exclude= to_exclude + list(np.where(to_plot_16[1,:] < -0.2)[0])\n",
    "len(to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusive_kmers=set(informative_kmers)-set(to_exclude)\n",
    "exclusive_kmers=list(exclusive_kmers)\n",
    "exclusive_kmers.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(to_exclude)/len(informative_kmers))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 k-mers are excluded whihc is 0.00015% of the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers[\">40%_in_each_class_only_no_outliers\"]=all_kmers.iloc[exclusive_kmers]\n",
    "ecombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\">40%_in_each_class_only_no_outliers\"][\"Kmer\"])]\n",
    "#Matrix of informative k-mers\n",
    "imatrices[\">40%_in_each_class_only_no_outliers\"]=ecombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First do standard scaling of the features\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "scaled_ecombined_total= StandardScaler().fit_transform(imatrices[\">40%_in_each_class_only_no_outliers\"])\n",
    "scaled_t_ecombined_total=scaled_ecombined_total.transpose().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_ml.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_16 = scaled_t_ecombined_total.compute_chunk_sizes()\n",
    "#When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "#n_samples and then divided by the singular values to ensure uncorrelated outputs with unit\n",
    "#component-wise variances.Whitening will remove some information from the transformed signal\n",
    "#(the relative variance scales of the components) but can sometime improve the predictive \n",
    "#accuracy of the downstream estimators by making their data respect some hard-wired assumptions\n",
    "pca_16 = PCA(n_components=2)\n",
    "to_plot_16=pca_16.fit(dX_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_SS_PCA.txt\", to_plot_16.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_16.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_16.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_16=np.genfromtxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_SS_PCA.txt\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=610\n",
    "y=5734\n",
    "z=10000\n",
    "\n",
    "#Make list of original indeces\n",
    "informative_kmers=set(informative_kmers)-set(to_exclude)\n",
    "informative_kmers=list(informative_kmers)\n",
    "informative_kmers.sort()\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(a,x)\n",
    "aS=random.sample(b,y)\n",
    "mM=random.sample(c,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_16.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_16.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_16[0,aM_corrected], y=to_plot_16[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_16[0,aS_corrected], y=to_plot_16[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_16[0,mM_corrected], y=to_plot_16[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Standard scaling + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + ' labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Exclusive kmers from the whole combined dataset without outliers',title_y=0.95)\n",
    "fig.write_html(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_SS_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization + PCA on exclusive k-mers without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "t_ecombined_total=imatrices[\">40%_in_each_class_only_no_outliers\"].transpose()\n",
    "transformer = Binarizer().fit(t_ecombined_total)  # fit does nothing.\n",
    "b_t_ecombined_total=transformer.transform(t_ecombined_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_17 = b_t_ecombined_total\n",
    "pca_17 = PCA(n_components=2)\n",
    "to_plot_17=pca_17.fit(dX_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_Binarization_PCA.txt\",\\\n",
    "           to_plot_17.components_, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_17.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_17.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_17=np.genfromtxt(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_Binarization_PCA.txt\",delimiter=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Select number of samples of each class to plot\n",
    "x=610\n",
    "y=5734\n",
    "z=10000\n",
    "\n",
    "#Make list of original indeces\n",
    "informative_kmers=set(informative_kmers)-set(to_exclude)\n",
    "informative_kmers=list(informative_kmers)\n",
    "informative_kmers.sort()\n",
    "indeces=informative_kmers\n",
    "\n",
    "#Select indeces that exists in each of the only datasets and in the subset \n",
    "aM=random.sample(a,x)\n",
    "aS=random.sample(b,y)\n",
    "mM=random.sample(c,z)\n",
    "\n",
    "#Dictionaries to change between old(original dataset) and new indeces(subset)\n",
    "zip_iterator = zip(list(range(0,to_plot_17.shape[1])),indeces)\n",
    "old2new = dict(zip_iterator)\n",
    "zip_iterator = zip(indeces,list(range(0,to_plot_17.shape[1])))\n",
    "new2old = dict(zip_iterator)\n",
    "\n",
    "aM_corrected=[new2old[x] for x in aM]\n",
    "aS_corrected=[new2old[x] for x in aS]\n",
    "mM_corrected=[new2old[x] for x in mM]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=to_plot_17[0,aM_corrected], y=to_plot_17[1,aM_corrected],\\\n",
    "                         showlegend=True,mode=\"markers\",legendgroup=\"aMetagenome\",name=\"aMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_17[0,aS_corrected], y=to_plot_17[1,aS_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"aGenome\",showlegend=True,name=\"aSGenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.add_trace(go.Scatter(x=to_plot_17[0,mM_corrected], y=to_plot_17[1,mM_corrected],mode=\"markers\",\\\n",
    "                         legendgroup=\"mMetagenome\",showlegend=True,name=\"mMetagenome\",\\\n",
    "                 marker=dict(opacity=0.5,size=5)))\n",
    "fig.update_layout(title='Binarization + PCA <br> Sample to plot: Random selection of '+ str(x) +\\\n",
    "                  ' kmers labelled as part of aMetagenomes, '+ str(y) + '<br>labelled as part of aSGenome '+\\\n",
    "                  str(z) + ' labelled as part of mMetagenomes'+\\\n",
    "                  '<br>Dataset: Exclusive kmers from the whole combined dataset without outliers',title_y=0.95)\n",
    "              \n",
    "fig.write_html(\"Preprocessed_data/combination/Kmers_ecombined_total_no_outliers_Binarization_PCA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imatrices=dict()\n",
    "ikmers=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRR_aSGenomes_HS_Oral=pd.read_csv(\"Metadata/aSGenomes/aSGenomes_HS_Oral_SRR.txt\",header=None,names=[\"SRR_accession\"])\n",
    "SRR_mMetagenomes_HS_Oral=pd.read_csv(\"Metadata/mMetagenomes/Oral_HS_mMetagenomes_SRR.txt\",\\\n",
    "                                     header=None,names=[\"SRR_accession\"])\n",
    "SRR_HS_Oral_aMetagenomes=SRR_HS_Oral[1:]\n",
    "SRR_HS_Oral_aSGenomes=list(SRR_aSGenomes_HS_Oral.SRR_accession)\n",
    "SRR_HS_Oral_mMetagenomes=list(SRR_mMetagenomes_HS_Oral.SRR_accession)\n",
    "names=SRR_HS_Oral+SRR_HS_Oral_aSGenomes+SRR_HS_Oral_mMetagenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1=pd.read_csv(\"Preprocessed_data/combination/classification_1.csv\")\n",
    "mMetagenomes_1=set(classification_1[(classification_1.Perc_mMetagenome > float(0))].index.values.tolist())\n",
    "aMetagenomes_1=set(classification_1[(classification_1.Perc_aMetagenome > float(0))].index.values.tolist())\n",
    "aSGenomes_1=set(classification_1[(classification_1.Perc_aSGenome > float(0))].index.values.tolist())\n",
    "intersection_1=set.intersection(aMetagenomes_1,mMetagenomes_1,aSGenomes_1)\n",
    "aMetagenomes_aSGenomes_1=aMetagenomes_1.intersection(aSGenomes_1)\n",
    "aMetagenomes_mMetagenomes_1=aMetagenomes_1.intersection(mMetagenomes_1)\n",
    "mMetagenomes_aSGenomes_1=mMetagenomes_1.intersection(aSGenomes_1)\n",
    "onlymMetagenomes_1=mMetagenomes_1-mMetagenomes_aSGenomes_1-aMetagenomes_mMetagenomes_1\n",
    "onlyaSGenomes_1=aSGenomes_1-mMetagenomes_aSGenomes_1-aMetagenomes_aSGenomes_1\n",
    "onlyaMetagenomes_1=aMetagenomes_1-aMetagenomes_aSGenomes_1-aMetagenomes_mMetagenomes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/list_of_indeces_of_RepresentativeSample.csv\",to_take,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take=np.genfromtxt(\"Classification/list_of_indeces_of_RepresentativeSample.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total=dd.read_table(\"kmMatrices/combination/combination_whole_matrix.txt\",sep=\" \",header=None,names=names)\n",
    "all_kmers=combined_total.Kmer.compute()\n",
    "all_kmers=all_kmers.reset_index()\n",
    "all_kmers=all_kmers.drop(\"index\",axis=1)\n",
    "ikmers[\"RepresentativeSample\"]=all_kmers.iloc[to_take]\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample\"][\"Kmer\"])]\n",
    "imatrices[\"RepresentativeSample\"]=icombined_total.drop(\"Kmer\",axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imatrices[\"RepresentativeSample\"]=imatrices[\"RepresentativeSample\"].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "ecombined_total=imatrices[\"RepresentativeSample\"]\n",
    "b_ecombined_total = Binarizer().fit_transform(ecombined_total)  # fit does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from sklearn.decomposition import PCA\n",
    "#Here I convert the np array into a Dask array using chunks of 1 GB\n",
    "dX_15 = b_ecombined_total.transpose()\n",
    "pca_15 = PCA(n_components=2)\n",
    "to_plot_15=pca_15.fit(dX_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Preprocessed_data/combination/RepresentativeSample_Binarization_PCA.txt\", to_plot_15.components_, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot_15.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt(\"Classification/RepresentativeSample_binarized.csv\", b_ecombined_total, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt=dd.read_table(\"Classification/RepresentativeSample_binarized.csv\",delimiter=\",\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ecombined_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt=attempt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1=Ancient\n",
    "#2=NonAncient\n",
    "to_take=one+two+three+four+five+six+seven\n",
    "labels=[]\n",
    "labels=labels+[False]*len(one)\n",
    "labels=labels+[False]*len(two)\n",
    "labels=labels+[True]*len(three)\n",
    "labels=labels+[False]*len(four)\n",
    "labels=labels+[True]*len(five)\n",
    "labels=labels+[False]*len(six)\n",
    "labels=labels+[True]*len(seven)\n",
    "Data = pd.DataFrame(list(zip(to_take, labels)),\n",
    "               columns =['Index', 'Label'])\n",
    "Data.sort_values(by='Index' , inplace=True)\n",
    "Data=Data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=Data.drop(\"index\",axis=1)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/Labels_RepresentativeSample.csv\",Data[\"Label\"], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_RepresentativeSample=np.genfromtxt(\"Classification/Labels_RepresentativeSample.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and testing with non-blockwise shuffling\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X=b_ecombined_total\n",
    "y=labels_RepresentativeSample\n",
    "#y=y.rechunk((55555, 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.5,shuffle=True,blockwise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    LR.fit(X_train, y_train)\n",
    "    y_test_predicted_LR=LR.predict(X_test)\n",
    "    y_train_predicted_LR=LR.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train, y_train_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_test_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train,y_train_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_test_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(labels_RepresentativeSample==False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(labels_RepresentativeSample==True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(np.where(y_train!=y_train_predicted_LR)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(y_train==y_train_predicted_LR)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score_LR=LR.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_score_LR)\n",
    "\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'ROC Curve for training set (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, y_train_score_LR)\n",
    "\n",
    "# Evaluating model performance at various thresholds\n",
    "df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr\n",
    "}, index=thresholds)\n",
    "df.index.name = \"Thresholds\"\n",
    "df.columns.name = \"Rate\"\n",
    "\n",
    "fig_thresh = px.line(\n",
    "    df, title='TPR and FPR at every threshold for training set',\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "fig_thresh.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig_thresh.update_xaxes(range=[0, 1], constrain='domain')\n",
    "fig_thresh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train,y_train_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_score_LR=LR.predict_proba(X_test)[:,1]\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_score_LR)\n",
    "\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'ROC Curve for test set (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_score_LR)\n",
    "\n",
    "# Evaluating model performance at various thresholds\n",
    "df = pd.DataFrame({\n",
    "    'False Positive Rate': fpr,\n",
    "    'True Positive Rate': tpr\n",
    "}, index=thresholds)\n",
    "df.index.name = \"Thresholds\"\n",
    "df.columns.name = \"Rate\"\n",
    "\n",
    "fig_thresh = px.line(\n",
    "    df, title='TPR and FPR at every threshold for test set',\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "fig_thresh.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig_thresh.update_xaxes(range=[0, 1], constrain='domain')\n",
    "fig_thresh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_test_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train,y_train_predicted_LR,target_names=[\"Non-ancient\",\"Ancient\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,y_test_predicted_LR,target_names=[\"Non-ancient\",\"Ancient\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/LR/y_test_predicted_labels.csv\",y_test_predicted_LR,delimiter=\",\")\n",
    "np.savetxt(\"Classification/LR/y_train_predicted_labels.csv\",y_train_predicted_LR,delimiter=\",\")\n",
    "np.savetxt(\"Classification/LR/y_test_true_labels.csv\",y_test,delimiter=\",\")\n",
    "np.savetxt(\"Classification/LR/y_train_true_labels.csv\",y_train,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/LR/y_train_score_LR.csv\",y_train_score_LR,delimiter=\",\")\n",
    "np.savetxt(\"Classification/LR/y_test_score_LR.csv\",y_test_score_LR,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_take_v=[]\n",
    "n=100000\n",
    "one_v=random.sample(intersection_1-set(to_take),int(0.456*n))\n",
    "two_v=random.sample(onlyaMetagenomes_1-set(to_take),int(59.275*n))\n",
    "three_v=random.sample(onlyaSGenomes_1-set(to_take),int(0.001*n))\n",
    "four_v=random.sample(onlymMetagenomes_1-set(to_take), int(16.103*n))\n",
    "five_v=random.sample(aMetagenomes_aSGenomes_1-intersection_1-set(to_take),int(4.745*n))\n",
    "six_v=random.sample(aMetagenomes_mMetagenomes_1-intersection_1-set(to_take),int(19.393*n))\n",
    "seven_v=random.sample(mMetagenomes_aSGenomes_1-intersection_1-set(to_take),int(0.026*n))\n",
    "to_take_v=one_v+two_v+three_v+four_v+five_v+six_v+seven_v\n",
    "to_take_v.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/list_of_indeces_of_RepresentativeSample_Validation.csv\",to_take_v,delimiter=\",\")\n",
    "\n",
    "to_take_v=np.genfromtxt(\"Classification/list_of_indeces_of_RepresentativeSample_Validation.csv\",delimiter=\",\")\n",
    "\n",
    "len(to_take_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikmers[\"RepresentativeSample_Validation\"]=all_kmers.iloc[to_take_v]\n",
    "icombined_total=combined_total[combined_total[\"Kmer\"].isin(ikmers[\"RepresentativeSample_Validation\"][\"Kmer\"])]\n",
    "imatrices[\"RepresentativeSample_Validation\"]=icombined_total.drop(\"Kmer\",axis=1).values\n",
    "\n",
    "imatrices[\"RepresentativeSamplen_Validation\"]=imatrices[\"RepresentativeSample_Validation\"].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "ecombined_total_v=imatrices[\"RepresentativeSample_Validation\"]\n",
    "b_ecombined_total_v = Binarizer().fit_transform(ecombined_total_v)  # fit does nothing.\n",
    "\n",
    "#attempt=dd.read_table(\"Classification/RepresentativeSample_binarized.csv\",delimiter=\",\").values\n",
    "\n",
    "b_ecombined_total_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1=Ancient\n",
    "#2=NonAncient\n",
    "to_take_v=one_v+two_v+three_v+four_v+five_v+six_v+seven_v\n",
    "labels=[]\n",
    "labels=labels+[False]*len(one_v)\n",
    "labels=labels+[False]*len(two_v)\n",
    "labels=labels+[True]*len(three_v)\n",
    "labels=labels+[False]*len(four_v)\n",
    "labels=labels+[True]*len(five_v)\n",
    "labels=labels+[False]*len(six_v)\n",
    "labels=labels+[True]*len(seven_v)\n",
    "Data = pd.DataFrame(list(zip(to_take_v, labels)),\n",
    "               columns =['Index', 'Label'])\n",
    "Data.sort_values(by='Index' , inplace=True)\n",
    "Data=Data.reset_index()\n",
    "\n",
    "Data=Data.drop(\"index\",axis=1)\n",
    "Data.head()\n",
    "\n",
    "np.savetxt(\"Classification/Labels_RepresentativeSample_Validation.csv\",Data[\"Label\"], delimiter=',')\n",
    "\n",
    "labels_RepresentativeSample_Validation=np.genfromtxt(\"Classification/Labels_RepresentativeSample_Validation.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v=b_ecombined_total_v\n",
    "y_v=labels_RepresentativeSample_Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "with joblib.parallel_backend('dask'):\n",
    "    y_v_predicted_LR=LR.predict(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_v,y_v_predicted_LR,target_names=[\"Non-ancient\",\"Ancient\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_v,y_v_predicted_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v_score_LR=LR.predict_proba(X_v)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/LR/y_v_score_LR.csv\",y_v_score_LR,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/LR/y_validation_true_labels.csv\",y_v,delimiter=\",\")\n",
    "np.savetxt(\"Classification/LR/y_validation_predicted_labels.csv\",y_v_predicted_LR,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDClassifier = SGDClassifier(n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    SGDClassifier.fit(X_train, y_train)\n",
    "    y_test_predicted_SGD=SGDClassifier.predict(X_test)\n",
    "    y_train_predicted_SGD=SGDClassifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, y_train_predicted_SGD))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_test_predicted_SGD))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_train,y_train_predicted_SGD))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test,y_test_predicted_SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Classification/SGD/y_test_predicted_labels.csv\",y_test_predicted_SGD,delimiter=\",\")\n",
    "np.savetxt(\"Classification/SGD/y_train_predicted_labels.csv\",y_train_predicted_SGD,delimiter=\",\")\n",
    "np.savetxt(\"Classification/SGD/y_test_true_labels.csv\",y_test,delimiter=\",\")\n",
    "np.savetxt(\"Classification/SGD/y_train_ture_labels.csv\",y_train,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "GPC = GaussianProcessClassifier(n_jobs=-1)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    GPC.fit(X_train, y_train)\n",
    "    y_test_predicted_GPC=GPC.predict(X_test)\n",
    "    y_train_predicted_GPC=GPC.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_train, y_train_predicted_GPC))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_test_predicted_GPC))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_train,y_train_predicted_GPC))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test,y_test_predicted_GPC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "              \"kernel\": ['rbf', 'poly', 'sigmoid'],\n",
    "              \"shrinking\": [True, False]}\n",
    "grid_search = GridSearchCV(SVC(gamma='auto', random_state=0, probability=True),\n",
    "                           param_grid=param_grid,\n",
    "                           return_train_score=False,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and testing with non-blockwise shuffling\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "X=b_ecombined_total\n",
    "y=labels_RepresentativeSample\n",
    "X_train_small, X_test_big, y_train_small, y_test_big = train_test_split(X, y,\\\n",
    "                                                        train_size=0.1,shuffle=True,blockwise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    grid_search.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
